{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"dirschema","text":"<p>A directory structure and metadata linter based on JSON Schema.</p> <p>JSON Schema is great for validating (files containing) JSON objects that e.g. contain metadata, but these are only the smallest pieces in the organization of a whole directory structure, e.g. of some dataset of project. When working on datasets of a certain kind, they might contain various types of data, each different file requiring different accompanying metadata, based on its file type and/or location.</p> <p>DirSchema combines JSON Schemas and regexes into a solution to enforce structural dependencies and metadata requirements in directories and directory-like archives. With it you can for example check that:</p> <ul> <li>only files of a certain type are in a location (e.g. only <code>jpg</code> files in directory <code>img</code>)</li> <li>for each data file there exists a metadata file (e.g. <code>test.jpg</code> has <code>test.jpg_meta.json</code>)</li> <li>each metadata file is valid according to some JSON Schema</li> </ul> <p>If validating these kinds of constraints looks appealing to you, this tool is for you!</p> <p>Dirschema features:</p> <ul> <li>Built-in support for schemas and metadata stored as JSON or YAML</li> <li>Built-in support for checking contents of ZIP and HDF5 archives</li> <li>Extensible validation interface for advanced needs beyond JSON Schema</li> <li>Both a Python library and a CLI tool to perform the validation</li> </ul>"},{"location":"#usage","title":"Usage","text":"<p>To get started, please check out the quickstart guide.</p>"},{"location":"#how-to-cite","title":"How to Cite","text":"<p>If you want to cite this project in your scientific work, please use the citation file in the repository.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>We kindly thank all authors and contributors.</p> <p></p> <p>This project was developed at the Institute for Materials Data Science and Informatics (IAS-9) of the J\u00fclich Research Center and funded by the Helmholtz Metadata Collaboration (HMC), an incubator-platform of the Helmholtz Association within the framework of the Information and Data Science strategic initiative.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>Here we provide notes that summarize the most important changes in each released version.</p> <p>Please consult the changelog to inform yourself about breaking changes and security issues.</p>"},{"location":"changelog/#0.1.0","title":"v0.1.0 (2023-05-08)","text":"<ul> <li>First PyPI release</li> </ul>"},{"location":"code_of_conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code_of_conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code_of_conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code_of_conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code_of_conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code_of_conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the project maintainers by e-mail. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code_of_conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code_of_conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code_of_conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code_of_conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code_of_conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code_of_conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"How To Contribute","text":"<p>All kinds of contributions are very welcome! You can contribute in various ways, e.g. by</p> <ul> <li>providing feedback</li> <li>asking questions</li> <li>suggesting ideas</li> <li>implementing features</li> <li>fixing problems</li> <li>improving documentation</li> </ul> <p>To make contributing to open source projects a good experience to everyone involved, please make sure that you follow our code of conduct when communicating with others.</p>"},{"location":"contributing/#ideas-questions-and-problems","title":"Ideas, Questions and Problems","text":"<p>If you have questions or difficulties using this software, please use the issue tracker.</p> <p>If your topic is not already covered by an existing issue, please create a new issue using one of the provided issue templates.</p> <p>If your issue is caused by incomplete, unclear or outdated documentation, we are also happy to get suggestions on how to improve it. Outdated or incorrect documentation is a bug, while missing documentation is a feature request.</p> <p>NOTE: If you want to report a critical security problem, do not open an issue! Instead, please create a private security advisory, or contact the current package maintainers directly by e-mail.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>This project uses Poetry for dependency management.</p> <p>You can run the following lines to check out the project and prepare it for development:</p> <pre><code>git clone git@github.com:Materials-Data-Science-and-Informatics/dirschema.git\ncd dirschema\npoetry install --with docs\npoetry run poe init-dev\n</code></pre> <p>Common tasks are accessible via poe:</p> <ul> <li> <p>Use <code>poetry run poe lint</code> to run linters manually, add <code>--all-files</code> to check everything.</p> </li> <li> <p>Use <code>poetry run poe test</code> to run tests, add <code>--cov</code> to also show test coverage.</p> </li> <li> <p>Use <code>poetry run poe docs</code> to generate local documentation</p> </li> </ul> <p>In order to contribute code, please open a pull request to the <code>dev</code> branch.</p> <p>Before opening the PR, please make sure that your changes</p> <ul> <li>are sufficiently covered by meaningful tests,</li> <li>are reflected in suitable documentation (API docs, guides, etc.), and</li> <li>successfully pass all pre-commit hooks.</li> </ul>"},{"location":"coverage/","title":"Coverage Report","text":""},{"location":"credits/","title":"Authors and Contributors","text":"<p>Main authors are persons whose contributions significantly shaped the state of the software at some point in time.</p> <p>Additional contributors are persons who are not main authors, but contributed non-trivially to this project, e.g. by providing smaller fixes and enhancements to the code and/or documentation.</p> <p>Of course, this is just a rough overview and categorization. For a more complete overview of all contributors and contributions, please inspect the git history of this repository.</p>"},{"location":"credits/#main-authors","title":"Main Authors","text":"<ul> <li>Anton Pirogov (     E-Mail,     ORCID   ): original author</li> </ul>"},{"location":"credits/#additional-contributors","title":"Additional Contributors","text":"<p>... maybe you?</p>"},{"location":"dev_guide/","title":"Developer Guide","text":"<p>This guide is targeting mainly developers, maintainers and other technical contributors and provides more information on how to work with this repository.</p>"},{"location":"dev_guide/#overview","title":"Overview","text":""},{"location":"dev_guide/#repository-structure","title":"Repository Structure","text":"<p>Here is a non-exhaustive list of the most important files and directories in the repository.</p> <p>General:</p> <ul> <li><code>AUTHORS.md</code>: acknowledges and lists all contributors</li> <li><code>CHANGELOG.md</code>: summarizes the changes for each version of the software for users</li> <li><code>CODE_OF_CONDUCT.md</code>: defines the social standards that must be followed by contributors</li> <li><code>CONTRIBUTING.md</code>: explains  how others can contribute to the project</li> <li><code>README.md</code>: provides an overview and points to other resources</li> </ul> <p>Metadata:</p> <ul> <li><code>CITATION.cff</code>: metadata stating how to cite the project</li> <li><code>codemeta.json</code>: metadata for harvesting by other tools and services</li> <li><code>LICENSE</code>: the (main) license of the project</li> <li><code>LICENSES</code>: copies of all licenses that apply to files in the project</li> <li><code>.reuse/dep5</code>: granular license and copyright information for all files and directories</li> </ul> <p>Development:</p> <ul> <li><code>pyproject.toml</code>: project metadata, dependencies, development tool configurations</li> <li><code>poetry.lock</code>: needed for reproducible installation of the project</li> <li><code>src</code>: actual code provided by the project</li> <li><code>tests</code>: all tests for the code in the project</li> <li><code>mkdocs.yml</code>: configuration of the project website</li> <li><code>docs</code>: most contents used for the project website</li> </ul> <p>Automation and Quality Control:</p> <ul> <li><code>.pre-commit-config.yaml</code>: quality assurance tools used in the project</li> <li><code>.github/workflows</code>: CI scripts for GitHub (QA, documentation and package deployment)</li> <li><code>.github/ISSUE_TEMPLATE</code>: templates for the GitHub issue tracker</li> <li><code>.gitlab-ci.yml</code>: mostly equivalent CI scripts, but for GitLab</li> <li><code>.gitlab/issue_templates</code>: The same issues templates, but for GitLab</li> </ul>"},{"location":"dev_guide/#used-tools","title":"Used Tools","text":"<p>Here is a non-exhaustive list of the most important tools used in the project.</p> <p>Best practices for modern Python development are implemented by using:</p> <ul> <li><code>poetry</code> for dependency management and packaging</li> <li><code>pytest</code> for unit testing</li> <li><code>hypothesis</code> for property-based testing</li> <li><code>pre-commit</code> for orchestrating linters, formatters and other utilities</li> <li><code>black</code> for source-code formatting</li> <li><code>autoflake</code> for automatically removing unused imports</li> <li><code>flake8</code> for general linting (using various linter plugins)</li> <li><code>pydocstyle</code> for checking docstring conventions</li> <li><code>interrogate</code> for computing docstring coverage</li> <li><code>mypy</code> for editor-independent type-checking</li> <li><code>mkdocs</code> for generating the project documentation website</li> <li><code>bandit</code> for checking security issues in the code</li> <li><code>safety</code> for checking security issues in the current dependencies</li> </ul> <p>Metadata best practices for FAIR software are implemented using:</p> <ul> <li><code>cffconvert</code> to check the <code>CITATION.cff</code> (citation metadata)</li> <li><code>codemetapy</code> to generate a <code>codemeta.json</code> (general software metadata)</li> <li><code>reuse</code> to check REUSE-compliance (granular copyright and license metadata)</li> <li><code>licensecheck</code> to scan for possible license incompatibilities in the dependencies</li> </ul>"},{"location":"dev_guide/#basics","title":"Basics","text":"<p>The project</p> <ul> <li>heavily uses <code>pyproject.toml</code>, which is a recommended standard</li> <li>adopts the <code>src</code> layout, to avoid common problems</li> <li>keeps the actual code (<code>src</code>) and test code (<code>tests</code>) separated</li> </ul> <p>The <code>pyproject.toml</code> is the main configuration file for the project. It contains both general information about the software as well as configuration for various tools.</p> <p>In older software, most of this information is often scattered over many little tool-specific configuration files and a <code>setup.py</code>, <code>setup.cfg</code> and/or <code>requirements.txt</code> file.</p> <p>In this project, <code>pyproject.toml</code> is the first place that should be checked when looking for the configuration of some development tool.</p>"},{"location":"dev_guide/#configuration","title":"Configuration","text":"<p>The main tool needed to manage and configure the project is Poetry.</p> <p>Please follow its setup documentation to install it correctly. Poetry should not be installed with <code>pip</code> like other Python tools.</p> <p>Poetry performs many important tasks:</p> <ul> <li>it manages the virtual environment(s) used for the project</li> <li>it manages all the dependencies needed for the code to work</li> <li>it takes care of packaging the code into a <code>pip</code>-installable package</li> </ul> <p>You can find a cheatsheet with the most important commands here and consult its official documentation for detailed information.</p> <p>Note that <code>poetry</code> is only needed for development of the repository. The end-users who just want to install and use this project do not need to set up or know anything about poetry.</p> <p>Note that if you use <code>poetry shell</code> and the project is installed with <code>poetry install</code>, in the following you do not have to prepend <code>poetry run</code> to commands you will see below.</p>"},{"location":"dev_guide/#task-runner","title":"Task Runner","text":"<p>It is a good practice to have a common way for launching different project-related tasks. It removes the need of remembering flags for various tools, and avoids duplication of the same commands in the CI pipelines. If something in a workflow needs to change, it can be changed in just one place, thus reducing the risk of making a mistake.</p> <p>Often projects use a shell script or <code>Makefile</code> for this purpose. This project uses poethepoet, as it integrates nicely with <code>poetry</code>. The tasks are defined in <code>pyproject.toml</code> and can be launched using:</p> <pre><code>poetry run poe TASK_NAME\n</code></pre>"},{"location":"dev_guide/#ci-workflows","title":"CI Workflows","text":"<p>The project contains CI workflows for both GitHub and GitLab.</p> <p>The main CI pipeline runs on each new pushed commit and will</p> <ol> <li>Run all configured code analysis tools,</li> <li>Run code tests with multiple versions of Python,</li> <li>build and deploy the online project documentation website, and</li> <li>if a new version tag was pushed, launch the release workflow</li> </ol>"},{"location":"dev_guide/#quality-control","title":"Quality Control","text":""},{"location":"dev_guide/#static-analysis","title":"Static Analysis","text":"<p>Except for code testing, most tools for quality control are added to the project as <code>pre-commit</code> hooks. The <code>pre-commit</code> tool takes care of installing, updating and running the tools according to the configuration in the <code>.pre-commit-config.yaml</code> file.</p> <p>For every new copy of the repository (e.g. after <code>git clone</code>), <code>pre-commit</code> first must be activated. This is usually done using <code>pre-commit install</code>, which also requires that <code>pre-commit</code> is already available. For more convenience, we simplified the procedure.</p> <p>In this project, you can run:</p> <pre><code>poetry run poe init-dev\n</code></pre> <p>This will make sure that <code>pre-commit</code> is enabled in your repository copy.</p> <p>Once enabled, every time you try to <code>git commit</code> some changed files various tools will run on those (and only those) files.</p> <p>This means that (with some exceptions) <code>pre-commit</code> by default will run only on the changed files that were added to the next commit (i.e., files in the git staging area). These files are usually colored in green when running <code>git status</code>.</p> <ul> <li>Some tools only report the problems they detected</li> <li>Some tools actively modify files (e.g., fix formatting)</li> </ul> <p>In any case, the <code>git commit</code> will fail if a file was modified by a tool, or some problems were reported. In order to complete the commit, you need to</p> <ul> <li>resolve all problems (by fixing them or marking them as false alarm), and</li> <li><code>git add</code> all changed files again (to update the files in the staging area).</li> </ul> <p>After doing that, you can retry to <code>git commit</code> your changes.</p> <p>To avoid having to deal with many issues at once, it is a good habit to run <code>pre-commit</code> by hand from time to time. In this project, this can be done with:</p> <pre><code>poetry run poe lint --all-files\n</code></pre>"},{"location":"dev_guide/#testing","title":"Testing","text":"<p>pytest is used as the main framework for testing.</p> <p>The project uses the <code>pytest-cov</code> plugin to integrate <code>pytest</code> with <code>coverage</code>, which collects and reports test coverage information.</p> <p>In addition to writing regular unit tests with <code>pytest</code>, consider using hypothesis, which integrates nicely with <code>pytest</code> and implements property-based testing - which involves automatic generation of randomized inputs for test cases. This can help to find bugs often found for various edge cases that are easy to overlook in ad-hoc manual tests. Such randomized tests can be a good addition to hand-crafted tests and inputs.</p> <p>To run all tests, either invoke <code>pytest</code> directly, or use the provided task:</p> <pre><code>poetry run poe test\n</code></pre>"},{"location":"dev_guide/#documentation","title":"Documentation","text":"<p>The project uses <code>mkdocs</code> with the popular and excellent <code>mkdocs-material</code> theme to generate the project documentation website, which provides both user and developer documentation.</p> <p><code>mkdocs</code> is configured in the <code>mkdocs.yml</code> file, which we prepared in a way that there is</p> <ul> <li>no need to duplicate sections from files in other places (such as <code>README.md</code>)</li> <li>fully automatic API documentation pages based on Python docstrings in the code</li> <li>a detailed test coverage report is included in the website</li> </ul> <p>The first point is important, because avoiding duplication means avoiding errors whenever text or examples are updated. The second point is convenient, as modules and functions do not need to be added by hand, which is easy to forget. The third point removes the need to use an external service such as CodeCov to store and present code coverage information.</p> <p>As software changes over time and users cannot always keep up with the latest developments, each new version of the software should provide version-specific documentation. To make this both possible as well as convenient, this project uses <code>mike</code> to generate and manage the <code>mkdocs</code> documentation for different versions of the software.</p>"},{"location":"dev_guide/#online-documentation","title":"Online Documentation","text":"<p>To avoid dependence on additional services such as readthedocs, the project website is deployed using GitHub Pages.</p> <p>The provided CI pipeline automatically generates the documentation for the latest development version (i.e., current state of the <code>main</code> branch) as well as every released version (i.e., marked by a version tag <code>vX.Y.Z</code>).</p>"},{"location":"dev_guide/#offline-documentation","title":"Offline Documentation","text":"<p>You can manually generate a local and fully offline copy of the documentation, which can be useful for e.g. previewing the results during active work on the documentation:</p> <pre><code>poetry install --with docs\npoetry run poe docs\n</code></pre> <p>Once the documentation site is built, run <code>mkdocs serve</code> and open <code>https://localhost:8000</code> in your browser to see the local copy of the website.</p>"},{"location":"dev_guide/#releases","title":"Releases","text":"<p>From time to time the project is ready for a new release for users.</p>"},{"location":"dev_guide/#creating-a-new-release","title":"Creating a New Release","text":"<p>Before releasing a new version, push the commit the new release should be based on to the upstream repository, and make sure that:</p> <ul> <li>the CI pipeline completes successfully</li> <li>the version number in <code>pyproject.toml</code> is updated, in particular:</li> <li>it must be larger than the previous released version</li> <li>it should adequately reflect the severity of changes</li> <li>the provided user and developer documentation is up-to-date, including:</li> <li>a new section in the <code>CHANGELOG.md</code> file summarizing changes in the new version</li> <li>possibly revised information about contributors and/or maintainers</li> </ul> <p>If this is the case, proceed with the release by:</p> <ul> <li>creating a new tag that matches the version in the <code>pyproject.toml</code>: <code>git tag vX.Y.Z</code></li> <li>pushing the new tag to the upstream repository: <code>git push origin vX.Y.Z</code></li> </ul> <p>The pushed version tag will trigger a pipeline that will:</p> <ul> <li>build and deploy the documentation website for the specific version</li> <li>publish the package to enabled targets (see below)</li> </ul>"},{"location":"dev_guide/#release-targets","title":"Release Targets","text":"<p>Targets for releases can be enabled or disabled in <code>.github/workflows/ci.yml</code> and configured by adapting the corresponding actions in <code>.github/workflows/releases.yml</code>.</p>"},{"location":"dev_guide/#github-release","title":"Github Release","text":"<p>By default, the release workflow will create a basic Github Release that provides a snapshot of the repository as a download. This requires no additional configuration.</p> <p>See here for information on how the Github release can be customized.</p> <p>Note that this release target is mostly for demonstration purposes. For most Python projects, using PyPI is the recommended primary distribution method.</p>"},{"location":"dev_guide/#pypi-and-compatible-package-indices","title":"PyPI (and compatible package indices)","text":"<p>For releases to PyPI and Test PyPI the project uses the new Trusted Publishers workflow that is both more secure and convenient to use than other authorization methods.</p> <p>Before the project can be released to PyPI or Test PyPI the first time, first a pending publisher must be added in the PyPI account of the main project maintainer, using <code>release.yml</code> as the requested workflow name.</p> <p>Once this is done, set the corresponding option (<code>to_pypi</code> / <code>to_test_pypi</code>) to <code>true</code> in the <code>publish</code> job in <code>ci.yml</code> to enable the corresponding publication target.</p> <p>If the old and less secure token-based authentication method is needed or the package should be published to a different PyPI-compatible package index, please adapt <code>release.yml</code> accordingly).</p>"},{"location":"license/","title":"License","text":"<p>Unless stated otherwise, all code provided by this project (excluding external dependencies) is distributed under the following license:</p> <pre><code>MIT License\n\nCopyright (c) 2021 Forschungszentrum J\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9) - Stefan Sandfeld &lt;s.sandfeld@fz-juelich.de&gt;\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n</code></pre> <p>This project is REUSE compliant. The following detailed license and copyright information in DEP5 format can also be found in the <code>.reuse/dep5</code> file in the project source directory:</p> <pre><code>Format: https://www.debian.org/doc/packaging-manuals/copyright-format/1.0/\nUpstream-Name: dirschema\nUpstream-Contact: Anton Pirogov &lt;a.pirogov@fz-juelich.de&gt;\nSource: https://github.com/Materials-Data-Science-and-Informatics/dirschema\n\nFiles: .gitignore pyproject.toml poetry.lock .pre-commit-config.yaml codemeta.json CITATION.cff README.md RELEASE_NOTES.md CHANGELOG.md CODE_OF_CONDUCT.md AUTHORS.md CONTRIBUTING.md .gitlab-ci.yml .gitlab/* .github/* mkdocs.yml docs/* schemas/*\nCopyright: 2021 Forschungszentrum J\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9) - Stefan Sandfeld &lt;s.sandfeld@fz-juelich.de&gt;\nLicense: CC0-1.0\n\nFiles: src/dirschema/* tests/*\nCopyright: 2021 Forschungszentrum J\u00fclich GmbH - Institute for Materials Data Science and Informatics (IAS9) - Stefan Sandfeld &lt;s.sandfeld@fz-juelich.de&gt;\nLicense: MIT\n</code></pre>"},{"location":"manual/","title":"DirSchema Manual","text":"<p>DirSchema was created to describe the structure of datasets as well as metadata requirements, under the assumption that the metadata for files is stored in separate JSON files that follow a reasonable and consistent naming convention throughout the dataset.</p> <p>For this purpose, it lifts JSON Schema validation from the level of individual JSON files to hierarchical directory-like structures, i.e. besides actual files and directories it is easily possible to use a DirSchema for validating archive files, like a HDF5 or ZIP file.</p>"},{"location":"manual/#adapter-interface","title":"Adapter Interface","text":"<p>A DirSchema can be evaluated on any kind of tree-shaped hierarchical entity that supports the following operations:</p> <ul> <li>return a list of paths to all \"files\" and \"directories\" (normalized as described below)</li> <li>test whether a path is a \"directory\" (inner node)</li> <li>test whether a path is a \"file\" (leaf)</li> <li>load a \"file\" (typically JSON) to perform JSON Schema or custom validation on it</li> </ul> <p>In the following we will use the language of directories and files and will refer to the whole tree structure as the dataset. Other directory/file-like structures can be processed if a suitable adapter implementing the required interface is provided.</p>"},{"location":"manual/#path-convention","title":"Path Convention","text":"<p>DirSchema rules are evaluated on a set of paths and rely on pattern matching to determine which rule(s) must be applied to which path. Therefore, it is important to understand how files and directories are uniquely mapped to paths.</p> <ul> <li>The set of paths in a dataset always contains at least the empty path     (representing the root directory)</li> <li>Furthermore, it contains all contained subdirectories and files     (except for ones that are ignored due to metadata convention (see later)     or adapter configuration (e.g. ignoring hidden files etc.)</li> </ul> <p>In order to have a unique representation of paths that can be used in regex patterns, all paths are normalized such that:</p> <ul> <li>each path is relative to the directory root (which is represented by the empty string)</li> <li>slashes are used to separate \"segments\"     (i.e. a sequence of directories, possibly ending with a file)</li> <li>each segment between two slashes is non-empty</li> <li>there is neither a leading nor a trailing slash (<code>/</code>)</li> <li>paths do not contain special \"file names\" like <code>.</code> (current dir) or <code>..</code> (parent dir)</li> </ul> <p>Example: <code>\"\"</code>, <code>\"a\"</code>, <code>\"a/b/c\"</code> are all valid paths as provided by the normalization</p>"},{"location":"manual/#metadata-convention","title":"Metadata Convention","text":"<p>JSON metadata can be provided for each file and directory. By default, it is assumed that for each file named <code>FILE</code> the metadata is located in a file named <code>FILE_meta.json</code>, whereas for a directory <code>DIR</code> the metadata is in <code>DIR/_meta.json</code>.</p> <p>The convention can be configured by overriding the prefixes and suffixes that are attached to the path itself and to the filename. The general pattern is as follows:</p> <p>For a path <code>a/b/c/d</code>, the metadata is located in:</p> <ul> <li><code>&lt;PATH_PREFIX/&gt;a/b/c&lt;/PATH_SUFFIX&gt;/&lt;FILE_PREFIX&gt;d&lt;FILE_SUFFIX&gt;</code> if <code>d</code> is a file</li> <li><code>&lt;PATH_PREFIX/&gt;a/b/c/d&lt;/PATH_SUFFIX&gt;/&lt;FILE_PREFIX&gt;&lt;FILE_SUFFIX&gt;</code> if <code>d</code> is a directory</li> </ul> <p>All these prefixes and suffixes are optional, except for the requirement that either a file prefix or a file suffix must be provided.</p> <p>All files following the used metadata naming convention are automatically excluded from the set of validated files. These files are seen as merely \"companion files\" to other files in the dataset. This simplifies the writing of DirSchemas, as otherwise these files would have to be excluded in an ad-hoc manner, which would fix the convention inside a DirSchema. Excluding them allows for changing the convention or using the DirSchema with datasets following different conventions, without changing the DirSchema itself.</p> <p>Example:</p> <p>With the default settings, the metadata file for <code>/a/b/c/d</code> is expected to be found at:</p> <ul> <li><code>/a/b/c/d_meta.json</code> if <code>d</code> is a file</li> <li><code>/a/b/c/d/_meta.json</code> if <code>d</code> is a directory</li> </ul> <p>If we would also add a path suffix equal to <code>metadata</code>, we would get:</p> <ul> <li><code>/a/b/c/metadata/d_meta.json</code> if <code>d</code> is a file</li> <li><code>/a/b/c/d/metadata/_meta.json</code> if <code>d</code> is a directory</li> </ul>"},{"location":"manual/#validation-by-json-schemas-and-custom-plugins","title":"Validation by JSON Schemas and custom plugins","text":"<p>In any context where JSON validation is to be performed and a schema can be provided, it is possible to supply one of the following in the corresponding location of the schema:</p> <ul> <li>a JSON Schema (directly embedded)</li> <li>an URI pointing to a JSON Schema</li> <li>a special URI pointing to a custom validation plugin</li> </ul> <p>For referencing JSON Schemas stored outside of the dirschema, the following possibilities exist:</p> <ul> <li>a <code>http(s)://</code> URI</li> <li>a <code>file://</code> URI or an absolute path (equivalent)</li> <li>a <code>local://</code> URI (resolved relative to the directory of the used dirschema by default)</li> <li>a <code>cwd://</code> URI (resolved relative to the current working directory)</li> <li>a relative path (treated as a <code>cwd://</code> path by default)</li> </ul> <p>To access a custom validation plugin, a pseudo-URI starting with <code>v#VALIDATOR://</code> is recognized, where <code>VALIDATOR</code> is a registered plugin.</p> <p>The <code>cwd://</code> URI is an explicit version that behaves like normal \"relative paths\", i.e. when the validation tool is launched in <code>/a/b</code>, a path <code>cwd://c/d</code> is expanded to <code>/a/b/c/d</code>.</p> <p>By default, <code>local://</code> URIs are expanded relative to the location of the main dirschema file. The reference directory for interpreting <code>local://</code> paths can also be overridden to resolve to an arbitrary different path supplied to the validator during initialization.</p> <p>Example:</p> <p>Consider the following setup:</p> <ul> <li>the dirschema lives in <code>/my/dirschemas/example.dirschema.yaml</code></li> <li>the dirschema validation is launched in directory <code>/my/workdir</code></li> <li>A custom validator called <code>myvalidator</code> is registered as a plugin</li> </ul> <p>Now let us see how the paths are resolved:</p> <ul> <li>A JSON Schema referenced as <code>https://www.example.org/schemas/some_schema.json</code>     remains unchanged (the schema will be downloaded)</li> <li>A JSON Schema referenced as <code>file:///schemas/some_schema.json</code>     remains unchanged</li> <li>A JSON Schema referenced as <code>cwd://schemas/some_schema.json</code>     expands to <code>file:///my/workdir/schemas/some_schema.json</code></li> <li>A JSON Schema referenced as <code>local://schemas/some_schema.json</code>     will expand to <code>file:///my/dirschemas/schemas/some_schema.json</code> by default     (or some other path, if the local base directory is overridden)</li> <li>A JSON Schema referenced as <code>/schemas/some_schema.json</code>     expands to <code>file:///schemas/some_schema.json</code></li> <li>A JSON Schema referenced as <code>schemas/some_schema.json</code>     expands to <code>file:///my/workdir/schemas/some_schema.json</code> by default     (if overridden, any prefix can be added to modify the interpretation of relative paths)</li> <li>A pseudo-URI <code>v#myvalidator://something</code> will call the validation plugin   with the current file or directory path and the string <code>something</code> as argument   (the argument can tell the plugin what kind of validation to perform or schema to use).</li> </ul> <p>Thus, custom validation plugins can be used to serve two purposes:</p> <ul> <li>perform validation beyond what is possible with JSON Schema</li> <li>still use JSON Schema internally, but allow to use JSON Schemas     that cannot be addressed using the built-in supported protocols</li> </ul> <p>Except for custom validation plugins, all these URIs and pseudo-URIs can be used also as values for <code>$ref</code> inside the dirschema or JSON Schemas. The custom plugin Pseudo-URIs may only be used with the corresponding validation keywords of DirSchema.</p> <p>Relative paths can be used for convenience throughout the schema and expanded to any builtin JSON Schema access protocol or custom validator by setting the relative schema base prefix when launching the validator. Notice that using a custom plugin prefix will break <code>$ref</code> resolving of relative paths (you should not use <code>$ref</code> without access protocol anyway). If you do it anyway and want relative paths to consistently be resolved as expected in <code>$ref</code>s, you must prefix the relative sub-schema location with <code>cwd://</code> or <code>local://</code> stating your intended semantics.</p> <p>While all the provided ways to refer to external schemas can be useful for applying dirschema in various contexts, consider mixing too many, especially multiple \"relative\" modes of accessing a validator or JSON Schema as a bad practice. It can make your schemas harder to understand and to reuse.</p>"},{"location":"manual/#dirschema-keywords","title":"DirSchema keywords","text":"<p>The keywords used in dirschema can be classified into some groups:</p> <ul> <li>Primitive rules: <code>type</code>, <code>valid</code>, <code>validMeta</code></li> </ul> <p>The primitive rules are those which perform the actual desired validation on a path.</p> <ul> <li>Logical connectives: <code>not</code>, <code>anyOf</code>, <code>allOf</code>, <code>oneOf</code></li> </ul> <p>The logical connectives work in the same way as in JSON Schema and are used to build more complex rules from the primitive rules.</p> <ul> <li>Syntactic sugar: <code>if</code>, <code>then</code>, <code>else</code></li> </ul> <p>Technically, <code>if</code>/<code>then</code>/<code>else</code> is redundant, as its complete behaviour can be replicated from logical connectives and suitable use of the <code>description</code> and <code>details</code> settings.</p> <p>Practically, it is added as syntactic sugar for the often needed case where a \"meta-level\" implication such as \"if precondition X is true, validate rule Y\" is desired, but the user should not be bothered with errors concerning violations of \"X\" because this is not a real validation error.</p> <p>To have more human-readable schemas and better error reporting, the guideline is to use <code>if/then/else</code> for rule selection and \"control flow\", whereas the logical connectives are to be used for actual complex validation rules.</p> <ul> <li>Pattern matching: <code>match</code>, <code>rewrite</code>, <code>next</code></li> </ul> <p>The pattern matching keywords are the mechanism for selecting which rules to apply to which paths and constructing relations between paths.</p> <ul> <li>Settings: <code>matchStart</code>, <code>matchStop</code>, <code>description</code>, <code>details</code></li> </ul> <p>The setting keywords affect the behaviour of the evaluation, but have no \"truth value\".</p>"},{"location":"manual/#dirschema-evaluation","title":"DirSchema Evaluation","text":"<p>When validating a dataset, the DirSchema is evaluated for each path individually and therefore rule violations are also reported for each path separately. For each path, the validation returns a (part of) the unsatisfied constraints as response. Rule evaluation proceeds recursively as follows.</p> <ol> <li>If a <code>match</code> key is present, the path is matched against the expression.</li> <li>Primitive constraints <code>type</code>, <code>valid</code> and <code>validMeta</code> are evaluated.</li> <li>Logical constraints <code>not</code>, <code>allOf</code>, <code>anyOf</code> and <code>oneOf</code> and <code>if/then/else</code> are evaluated.</li> <li>The <code>next</code> rule is evaluated on the path (possibly rewritten by <code>rewrite</code>), if present.</li> </ol> <p>Whenever one of these stages fails, the evaluation of the current rule is aborted. In the following, all available constraints and other keys are explained in more detail.</p>"},{"location":"manual/#dirschema-rules","title":"DirSchema Rules","text":"<p>A DirSchema rule is - similar to a JSON Schema - either a boolean (rule that is trivially <code>true</code> or <code>false</code>), or a conjunction of at most one of each kind of possible primitive and/or complex constraints. A constraint is primitive iff it does not contain any nested constraint (i.e. primitive rules are leaves in the tree of nested rules).</p> <p>DirSchema rules are assumed to be JSON or YAML files. In the following it is assumed that JSON and YAML syntax is understood and only the key/value pairs for defining constraints are presented.</p>"},{"location":"manual/#matching-and-rewriting","title":"Matching and Rewriting","text":"<p>As explained above, the complete rule expression is evaluated on each path. To apply different rules to different paths and express dependencies between related paths, DirSchema provides regex matching and substitution for paths.</p>"},{"location":"manual/#match","title":"match","text":"<p>Value: string (containing a regex pattern)</p> <p>Description: Require that the path must fully match the provided regex.</p> <p>If the match fails, it is assumed that the current rule is not intended for the current path and therefore further evaluation of this rule is aborted.</p> <p>The behavior of <code>match</code> can be modified by setting <code>matchStart</code> and/or <code>matchStop</code> to restrict the matching scope to certain path segments. Such an interval is called path slice.</p> <p>For example, given the path <code>a/b/c/d</code> with <code>matchStart: 1</code> and <code>matchStop: -1</code>, the match (and possible rewrite) is performed only on the path slice <code>b/c</code>.</p> <p>Capture groups (defined by parentheses in the regex) can be used for the <code>rewrite</code> in the current or any nested rule, unless overridden by a different <code>match</code>.</p>"},{"location":"manual/#matchstart","title":"matchStart","text":"<p>Value: integer (default: 0)</p> <p>Description: Defines the index of the first path segment to be included in the match.</p> <p>Negative indices work the same as in Python.</p> <p>For example, to match only in the file name, <code>matchStart</code> can be set to <code>-1</code>.</p> <p>This setting is inherited into contained rules until overridden.</p>"},{"location":"manual/#matchstop","title":"matchStop","text":"<p>Value: integer (default: 0)</p> <p>Description: Defines the index of the first path segment after <code>matchStart</code> that is not to be included in the match.</p> <p>Negative indices work the same as in Python.</p> <p>Contrary to Python, a value of 0 means \"until the end\", like leaving out the end index in a Python slice.</p> <p>This setting is inherited into contained rules until overridden.</p>"},{"location":"manual/#rewrite","title":"rewrite","text":"<p>Value: string (substitution, possibly containing capture references)</p> <p>Description:</p> <p>Rewrite (parts of) the current path.</p> <p>The rewritten path is used instead of the current path in the <code>next</code> rule, all constraints on the same level as the rewrite are evaluated on the original path! Therefore having a <code>rewrite</code> without a <code>next</code> rule has no effect.</p> <p>Capture groups of the most recent <code>match</code> (i.e. on the same or level or in an ancestor rule) can be used in the substitution. If there is no applicable <code>match</code>, a default match for the pattern <code>(.*)</code> is assumed and therefore <code>\\\\1</code> references the whole matched path or path slice (determined by the currently active <code>matchStart</code>/<code>matchStop</code>).</p> <p>In principle, this can be used to roughly emulate the functionality of <code>validMeta</code>, but as metadata requirements are one of the main use cases, validMeta is preferable, as it is not hard-coding a metadata file naming convention.</p> <p>But in a case where more than one metadata file is required for a single file, the non-standard file could be validated by a combination of <code>rewrite</code> and <code>valid</code>, if there is no other way to express the desired constraints.</p>"},{"location":"manual/#primitive-rules","title":"Primitive Rules","text":"<p>Beside <code>match</code>, the following primitive rules are provided:</p>"},{"location":"manual/#type","title":"type","text":"<p>Value: boolean, \"file\" or \"dir\"</p> <p>Description: Require that the path:</p> <ul> <li><code>true</code>: exists (either file or directory)</li> <li><code>false</code>: does not exist</li> <li><code>\"file\"</code>: is a file</li> <li><code>\"dir\"</code>: is a directory</li> </ul>"},{"location":"manual/#valid","title":"valid","text":"<p>Value: JSON Schema or string</p> <p>Description: Require that the path is loadable as JSON by the used adapter and is successfully validated by the referenced JSON Schema or custom validator.</p> <p>Validation fails if the path does not exist, cannot be loaded by the adapter or is not valid according to the validation handler.</p>"},{"location":"manual/#validmeta","title":"validMeta","text":"<p>Value: JSON Schema or string</p> <p>Description: Require that the metadata file of the current path (according to the used convention) is loadable as JSON by the used adapter and is successfully validated by the referenced JSON Schema or custom validator.</p> <p>Validation fails if the path does not exist, the metadata companion file does not exist, the metadata file cannot be loaded by the adapter or is not valid according to the validation handler.</p>"},{"location":"manual/#combinations-of-rules","title":"Combinations of Rules","text":"<p>To build more complex rules, DirSchema provides the same logical connectives that can be used with JSON Schema. Additionally, an implication keyword <code>next</code> is provided explicitly and described further below.</p> <p>Notice that contrary to typical logical semantics (and just as in JSON Schema), <code>oneOf/anyOf</code> evaluate to true for empty arrays, because they are interpreted as \"not existing\" instead of being treated as empty existentials.</p> <p>For each path, the rules are checked in the listed order (\"short circuiting\"), which matters for <code>anyOf</code> - once a rule in the array is satisfied, the following rules are not evaluated. So prefer putting simpler/the most common case first.</p>"},{"location":"manual/#not","title":"not","text":"<p>Value: DirSchema</p> <p>Description: Logical negation.</p>"},{"location":"manual/#allof","title":"allOf","text":"<p>Value: Array of DirSchema</p> <p>Description: Logical conjunction.</p>"},{"location":"manual/#anyof","title":"anyOf","text":"<p>Value: Array of DirSchema</p> <p>Description: Logical disjunction.</p>"},{"location":"manual/#oneof","title":"oneOf","text":"<p>Value: Array of DirSchema</p> <p>Description: Satisfied if exactly one rule in the array of DirSchemas is satisfied.</p>"},{"location":"manual/#next","title":"next","text":"<p>Value: DirSchema</p> <p>Description: If all other constraints in the current rule are satisfied, require that the rule provided in the value is also satisfied on the (possibly rewritten) path.</p> <p>This mechanism exists first and foremost in order to be used in combination with <code>rewrite</code>, as just combining multiple rules can be achieved using <code>allOf</code>.</p> <p>Additionally, this can be used for sequential \"short circuiting\" of rule evaluation to modify or refine the four evaluation phases outlined above.</p>"},{"location":"manual/#if-then-else","title":"if-then-else","text":""},{"location":"manual/#if","title":"if","text":"<p>Value: DirSchema</p> <p>Description: If specified, will be evaluated on current path. Depending on result, either the <code>then</code> or the <code>else</code> rule will be evaluated.</p>"},{"location":"manual/#then","title":"then","text":"<p>Value: DirSchema</p> <p>Description: If given, must be satisfied in case that the <code>if</code> rule is satisfied.</p>"},{"location":"manual/#else","title":"else","text":"<p>Value: DirSchema</p> <p>Description: If given, must be satisfied in case that the <code>if</code> rule is violated.</p>"},{"location":"manual/#error-reporting","title":"Error reporting","text":""},{"location":"manual/#description","title":"description","text":"<p>Value: string</p> <p>Description: If given, will override all other error messages from immediate child keys of this rule. To completely silence errors from this rule, set to empty string.</p> <p>If you want to have multiple custom error messages for keys in this rule (e.g. checking both <code>type</code> and <code>validMeta</code> with separate error messages), move these keys into <code>allOf</code>, and add individual <code>description</code> strings to the sub-rules inside <code>allOf</code>.</p>"},{"location":"manual/#details","title":"details","text":"<p>Value: boolean (true by default)</p> <p>Description:  If true, will preserve error messages reported from nested sub-rules e.g. from logical connectives etc. If false, will discard them. This can be used in combination with <code>description</code> to provide higher-level errors for logically complex rules where the default error report is not helpful.</p>"},{"location":"manual/#modularity","title":"Modularity","text":"<p>In any place where a DirSchema or JSON Schema is expected, one can also use <code>$ref</code> to reference them, both in YAML as well as JSON format, located at a remote or local location. This works for all supported protocols except for custom validation plugins (i.e. custom validator pseudo-URIs are only permitted as values for <code>valid</code> and <code>validMeta</code>).</p>"},{"location":"manual/#examples","title":"Examples","text":"<p>TODO:</p> <p>Show non-trivial example for match slice/rewrite and scoping</p> <p>Show example how next can be used for short circuiting</p> <p>Show mutex example?</p>"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<pre><code>pip install dirschema\n</code></pre>"},{"location":"quickstart/#getting-started","title":"Getting Started","text":"<p>The <code>dirschema</code> tool needs as input:</p> <ul> <li>a DirSchema YAML file (containing a specification), and</li> <li>a path to a directory or file (e.g. zip file) that should be checked.</li> </ul> <p>You can run it like this:</p> <pre><code>dirschema my_dirschema.yaml DIRECTORY_OR_ARCHIVE_PATH\n</code></pre> <p>If the validation was successful, there will be no output. Otherwise, the tool will output a list of errors (e.g. invalid metadata, missing files, etc.).</p> <p>You can also use <code>dirschema</code> from other Python code as a library:</p> <pre><code>from dirschema.validate import DSValidator\nDSValidator(\"/path/to/dirschema\").validate(\"/dataset/path\")\n</code></pre> <p>Similarly, the method will return an error dict, which will be empty if the validation succeeded.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>dirschema<ul> <li>adapters</li> <li>cli</li> <li>core</li> <li>json<ul> <li>handler</li> <li>handler_pydantic</li> <li>handlers</li> <li>parse</li> <li>validate</li> </ul> </li> <li>log</li> <li>validate</li> </ul> </li> </ul>"},{"location":"reference/dirschema/","title":"dirschema","text":"<p>Top level module of the project.</p>"},{"location":"reference/dirschema/adapters/","title":"adapters","text":"<p>Interface to perform DirSchema validation on various directory-like structures.</p>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory","title":"IDirectory","text":"<p>             Bases: <code>ABC</code></p> <p>Abstract interface for things that behave like directories and files.</p> <p>An adapter is intended to be instantiated mainly using <code>for_path</code>, based on a regular path in the file system.</p> <p>Use the constructor to initialize an adapter for a more general data source (such as an object to work with an open archive file, etc.)</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>class IDirectory(ABC):\n\"\"\"Abstract interface for things that behave like directories and files.\n\n    An adapter is intended to be instantiated mainly using `for_path`,\n    based on a regular path in the file system.\n\n    Use the constructor to initialize an adapter for a more general data\n    source (such as an object to work with an open archive file, etc.)\n    \"\"\"\n\n    @abstractmethod\n    def __init__(cls, obj: object) -&gt; None:\n\"\"\"Initialize an instance for a suitable directory-like object.\"\"\"\n\n    @classmethod\n    @abstractmethod\n    def for_path(cls, path: Path):\n\"\"\"Return an instance for a path to a archive file or a directory.\n\n        Args:\n            path: Path to a file or directory compatible with the adapter.\n        \"\"\"\n\n    @abstractmethod\n    def get_paths(self) -&gt; Iterable[str]:\n\"\"\"Return paths relative to root directory that are to be checked.\"\"\"\n\n    @abstractmethod\n    def is_dir(self, path: str) -&gt; bool:\n\"\"\"Return whether the path is (like) a directory.\"\"\"\n\n    @abstractmethod\n    def is_file(self, path: str) -&gt; bool:\n\"\"\"Return whether the path is (like) a file.\"\"\"\n\n    @abstractmethod\n    def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"Try loading data from file at given path (to perform validation on it).\"\"\"\n\n    def decode_json(self, data: IO[bytes], path: str) -&gt; Optional[Any]:\n\"\"\"Try parsing binary data stream as JSON (to perform validation on it).\n\n        Second argument is the path of the opened stream.\n\n        Default implementation will first try parsing as JSON, then as YAML.\n        \"\"\"\n        try:\n            return json.load(data)\n        except json.JSONDecodeError:\n            try:\n                return yaml.load(data)\n            except yaml_parser.ParserError:\n                return None\n\n    def load_meta(self, path: str) -&gt; Optional[Any]:\n\"\"\"Use open_file and decode_json to load JSON metadata.\"\"\"\n        f = self.open_file(path)\n        return self.decode_json(f, path) if f is not None else None\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.__init__","title":"__init__  <code>abstractmethod</code>","text":"<pre><code>__init__(obj: object) -&gt; None\n</code></pre> <p>Initialize an instance for a suitable directory-like object.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@abstractmethod\ndef __init__(cls, obj: object) -&gt; None:\n\"\"\"Initialize an instance for a suitable directory-like object.\"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.for_path","title":"for_path  <code>abstractmethod</code> <code>classmethod</code>","text":"<pre><code>for_path(path: Path)\n</code></pre> <p>Return an instance for a path to a archive file or a directory.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to a file or directory compatible with the adapter.</p> required Source code in <code>src/dirschema/adapters.py</code> <pre><code>@classmethod\n@abstractmethod\ndef for_path(cls, path: Path):\n\"\"\"Return an instance for a path to a archive file or a directory.\n\n    Args:\n        path: Path to a file or directory compatible with the adapter.\n    \"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.get_paths","title":"get_paths  <code>abstractmethod</code>","text":"<pre><code>get_paths() -&gt; Iterable[str]\n</code></pre> <p>Return paths relative to root directory that are to be checked.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@abstractmethod\ndef get_paths(self) -&gt; Iterable[str]:\n\"\"\"Return paths relative to root directory that are to be checked.\"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.is_dir","title":"is_dir  <code>abstractmethod</code>","text":"<pre><code>is_dir(path: str) -&gt; bool\n</code></pre> <p>Return whether the path is (like) a directory.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@abstractmethod\ndef is_dir(self, path: str) -&gt; bool:\n\"\"\"Return whether the path is (like) a directory.\"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.is_file","title":"is_file  <code>abstractmethod</code>","text":"<pre><code>is_file(path: str) -&gt; bool\n</code></pre> <p>Return whether the path is (like) a file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@abstractmethod\ndef is_file(self, path: str) -&gt; bool:\n\"\"\"Return whether the path is (like) a file.\"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.open_file","title":"open_file  <code>abstractmethod</code>","text":"<pre><code>open_file(path: str) -&gt; Optional[IO[bytes]]\n</code></pre> <p>Try loading data from file at given path (to perform validation on it).</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@abstractmethod\ndef open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"Try loading data from file at given path (to perform validation on it).\"\"\"\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.decode_json","title":"decode_json","text":"<pre><code>decode_json(data: IO[bytes], path: str) -&gt; Optional[Any]\n</code></pre> <p>Try parsing binary data stream as JSON (to perform validation on it).</p> <p>Second argument is the path of the opened stream.</p> <p>Default implementation will first try parsing as JSON, then as YAML.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def decode_json(self, data: IO[bytes], path: str) -&gt; Optional[Any]:\n\"\"\"Try parsing binary data stream as JSON (to perform validation on it).\n\n    Second argument is the path of the opened stream.\n\n    Default implementation will first try parsing as JSON, then as YAML.\n    \"\"\"\n    try:\n        return json.load(data)\n    except json.JSONDecodeError:\n        try:\n            return yaml.load(data)\n        except yaml_parser.ParserError:\n            return None\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.IDirectory.load_meta","title":"load_meta","text":"<pre><code>load_meta(path: str) -&gt; Optional[Any]\n</code></pre> <p>Use open_file and decode_json to load JSON metadata.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def load_meta(self, path: str) -&gt; Optional[Any]:\n\"\"\"Use open_file and decode_json to load JSON metadata.\"\"\"\n    f = self.open_file(path)\n    return self.decode_json(f, path) if f is not None else None\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir","title":"RealDir","text":"<p>             Bases: <code>IDirectory</code></p> <p>Pass-through implementation for working with actual file system.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>class RealDir(IDirectory):\n\"\"\"Pass-through implementation for working with actual file system.\"\"\"\n\n    def __init__(self, path: Path) -&gt; None:\n\"\"\"Initialize adapter for a plain directory path.\"\"\"\n        self.base = path\n\n    @classmethod\n    def for_path(cls, path: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n        return cls(path)\n\n    def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n        paths = filter(lambda p: not p.is_symlink(), sorted(self.base.rglob(\"*\")))\n        return itertools.chain(\n            [\"\"], map(lambda p: str(p.relative_to(self.base)), paths)\n        )\n\n    def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n        try:\n            return open(self.base / path, \"rb\")\n        except (FileNotFoundError, IsADirectoryError):\n            return None\n\n    def is_dir(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n        return (self.base / dir).is_dir()\n\n    def is_file(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n        return (self.base / dir).is_file()\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.__init__","title":"__init__","text":"<pre><code>__init__(path: Path) -&gt; None\n</code></pre> <p>Initialize adapter for a plain directory path.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def __init__(self, path: Path) -&gt; None:\n\"\"\"Initialize adapter for a plain directory path.\"\"\"\n    self.base = path\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.for_path","title":"for_path  <code>classmethod</code>","text":"<pre><code>for_path(path: Path)\n</code></pre> <p>See dirschema.adapters.IDirectory.for_path.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@classmethod\ndef for_path(cls, path: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n    return cls(path)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.get_paths","title":"get_paths","text":"<pre><code>get_paths() -&gt; Iterable[str]\n</code></pre> <p>See dirschema.adapters.IDirectory.get_paths.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n    paths = filter(lambda p: not p.is_symlink(), sorted(self.base.rglob(\"*\")))\n    return itertools.chain(\n        [\"\"], map(lambda p: str(p.relative_to(self.base)), paths)\n    )\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.open_file","title":"open_file","text":"<pre><code>open_file(path: str) -&gt; Optional[IO[bytes]]\n</code></pre> <p>See dirschema.adapters.IDirectory.open_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n    try:\n        return open(self.base / path, \"rb\")\n    except (FileNotFoundError, IsADirectoryError):\n        return None\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.is_dir","title":"is_dir","text":"<pre><code>is_dir(dir: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_dir.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_dir(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n    return (self.base / dir).is_dir()\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.RealDir.is_file","title":"is_file","text":"<pre><code>is_file(dir: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_file(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n    return (self.base / dir).is_file()\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir","title":"ZipDir","text":"<p>             Bases: <code>IDirectory</code></p> <p>Adapter for working with zip files (otherwise equivalent to <code>RealDir</code>).</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>class ZipDir(IDirectory):\n\"\"\"Adapter for working with zip files (otherwise equivalent to `RealDir`).\"\"\"\n\n    def __init__(self, zip_file: zip.ZipFile):\n\"\"\"Initialize adapter for a zip file.\"\"\"\n        self.file: zip.ZipFile = zip_file\n        self.names: Set[str] = set(self.file.namelist())\n        self.names.add(\"/\")\n\n    @classmethod\n    def for_path(cls, path: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n        opened = zip.ZipFile(path, \"r\")  # auto-closed on GC, no need to do anything\n        return cls(opened)\n\n    def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n        return itertools.chain(map(lambda s: s.rstrip(\"/\"), sorted(self.names)))\n\n    def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n        try:\n            return self.file.open(path)\n        except (KeyError, IsADirectoryError):\n            return None\n\n    # as is_dir and is_file of zip.Path appear to work purely syntactically,\n    # they're useless for us. We rather just lookup in the list of paths we need anyway\n\n    def is_dir(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n        cand_name: str = dir.rstrip(\"/\") + \"/\"\n        return cand_name in self.names\n\n    def is_file(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n        cand_name: str = dir.rstrip(\"/\")\n        return cand_name in self.names\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.__init__","title":"__init__","text":"<pre><code>__init__(zip_file: zip.ZipFile)\n</code></pre> <p>Initialize adapter for a zip file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def __init__(self, zip_file: zip.ZipFile):\n\"\"\"Initialize adapter for a zip file.\"\"\"\n    self.file: zip.ZipFile = zip_file\n    self.names: Set[str] = set(self.file.namelist())\n    self.names.add(\"/\")\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.for_path","title":"for_path  <code>classmethod</code>","text":"<pre><code>for_path(path: Path)\n</code></pre> <p>See dirschema.adapters.IDirectory.for_path.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@classmethod\ndef for_path(cls, path: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n    opened = zip.ZipFile(path, \"r\")  # auto-closed on GC, no need to do anything\n    return cls(opened)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.get_paths","title":"get_paths","text":"<pre><code>get_paths() -&gt; Iterable[str]\n</code></pre> <p>See dirschema.adapters.IDirectory.get_paths.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n    return itertools.chain(map(lambda s: s.rstrip(\"/\"), sorted(self.names)))\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.open_file","title":"open_file","text":"<pre><code>open_file(path: str) -&gt; Optional[IO[bytes]]\n</code></pre> <p>See dirschema.adapters.IDirectory.open_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n    try:\n        return self.file.open(path)\n    except (KeyError, IsADirectoryError):\n        return None\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.is_dir","title":"is_dir","text":"<pre><code>is_dir(dir: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_dir.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_dir(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n    cand_name: str = dir.rstrip(\"/\") + \"/\"\n    return cand_name in self.names\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.ZipDir.is_file","title":"is_file","text":"<pre><code>is_file(dir: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_file(self, dir: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n    cand_name: str = dir.rstrip(\"/\")\n    return cand_name in self.names\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir","title":"H5Dir","text":"<p>             Bases: <code>IDirectory</code></p> <p>Adapter for working with HDF5 files.</p> <p>Attributes do not fit nicely into the concept of just directories and files. The following conventions are used to checking attributes:</p> <p>An attribute 'attr' of some dataset or group '/a/b' is mapped to the path '/a/b@attr' and is interpreted as a file.</p> <p>Therefore, '@' MUST NOT be used in names of groups, datasets or attributes.</p> <p>Only JSON is supported for the metadata, assuming that HDF5 files are usually not constructed by hand (which is the main reason for YAML support in the other cases).</p> <p>All stored metadata entities must have a name ending with \".json\" in order to distinguish them from plain strings. This is done because datasets or attributes are often used for storing simple values that could also be validated using a JSON Schema.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>class H5Dir(IDirectory):\n\"\"\"Adapter for working with HDF5 files.\n\n    Attributes do not fit nicely into the concept of just directories and files.\n    The following conventions are used to checking attributes:\n\n    An attribute 'attr' of some dataset or group '/a/b'\n    is mapped to the path '/a/b@attr' and is interpreted as a file.\n\n    Therefore, '@' MUST NOT be used in names of groups, datasets or attributes.\n\n    Only JSON is supported for the metadata, assuming that HDF5 files are usually not\n    constructed by hand (which is the main reason for YAML support in the other cases).\n\n    All stored metadata entities must have a name ending with \".json\"\n    in order to distinguish them from plain strings. This is done because datasets\n    or attributes are often used for storing simple values that could also be\n    validated using a JSON Schema.\n    \"\"\"\n\n    _ATTR_SEP = \"@\"\n\"\"\"Separator used in paths to separate a HDF5 node from an attribute.\"\"\"\n\n    _JSON_SUF = \".json\"\n\"\"\"Suffix used in leaf nodes to distinguish strings from JSON-serialized data.\"\"\"\n\n    def __init__(self, hdf5_file: h5py.File) -&gt; None:\n\"\"\"Initialize adapter for a HDF5 file.\"\"\"\n        self.file: h5py.File = hdf5_file\n\n    @classmethod\n    def for_path(cls, dir: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n        _require_h5py()\n        opened = h5py.File(dir, \"r\")  # auto-closed on GC, no need to do anything\n        return cls(opened)\n\n    def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n        ret = [\"\"]\n        for atr in self.file[\"/\"].attrs.keys():\n            ret.append(f\"{self._ATTR_SEP}{atr}\")\n\n        def collect(name: str) -&gt; None:\n            if name.find(self._ATTR_SEP) &gt;= 0:\n                raise ValueError(f\"Invalid name, must not contain {self._ATTR_SEP}!\")\n            ret.append(name)\n            for atr in self.file[name].attrs.keys():\n                ret.append(f\"{name}{self._ATTR_SEP}{atr}\")\n\n        self.file.visit(collect)\n        return ret\n\n    def is_dir(self, path: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n        if path == \"\":\n            return True  # root directory\n        if path.find(self._ATTR_SEP) &gt;= 0 or path not in self.file:\n            return False  # not existing or is an attribute\n        if isinstance(self.file[path], h5py.Group):\n            return True  # is a group\n        return False  # something that exists, but is not a group\n\n    def is_file(self, path: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n        # attributes (treated like special files) exist\n        # if underlying group/dataset exists\n        if path.find(self._ATTR_SEP) &gt;= 0:\n            p = path.split(self._ATTR_SEP)\n            p[0] = p[0] or \"/\"\n            return p[0] in self.file and p[1] in self.file[p[0]].attrs\n        else:\n            # otherwise check it is a dataset (= \"file\")\n            return path in self.file and isinstance(self.file[path], h5py.Dataset)\n\n    def decode_json(self, data: IO[bytes], path: str) -&gt; Optional[Any]:\n\"\"\"See [dirschema.adapters.IDirectory.decode_json][].\"\"\"\n        bs = data.read()\n        try:\n            ret = json.loads(bs)\n        except json.JSONDecodeError:\n            return None\n\n        if isinstance(ret, dict) and not path.endswith(self._JSON_SUF):\n            return bs\n        else:\n            return ret\n\n    def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n        p = path\n        if p.find(self._ATTR_SEP) &gt;= 0:\n            # try treating as attribute, return data if it is a string\n            f, s = p.split(self._ATTR_SEP)\n            f = f or \"/\"\n            if f in self.file and s in self.file[f].attrs:\n                dat = self.file[f].attrs[s]\n                if isinstance(dat, h5py.Empty):\n                    return None\n                if isinstance(dat, str):\n                    if not path.endswith(self._JSON_SUF):\n                        dat = f'\"{dat}\"'  # JSON-encoded string\n                else:\n                    dat = json.dumps(dat.tolist())\n                return io.BytesIO(dat.encode(\"utf-8\"))\n            else:\n                return None\n\n        # check that the path exists and is a dataset, but not a numpy array\n        if p not in self.file:\n            return None\n\n        dat = self.file[p]\n        if not isinstance(dat, h5py.Dataset):\n            return None\n\n        bs: Any = dat[()]\n        if isinstance(bs, numpy.ndarray):\n            return None\n\n        # the only kinds of datasets we accept are essentially utf-8 strings\n        # which are represented as possibly wrapped bytes\n        if isinstance(bs, numpy.void):  # void-wrapped bytes -&gt; unpack\n            bs = bs.tobytes()\n\n        return io.BytesIO(bs)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.__init__","title":"__init__","text":"<pre><code>__init__(hdf5_file: h5py.File) -&gt; None\n</code></pre> <p>Initialize adapter for a HDF5 file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def __init__(self, hdf5_file: h5py.File) -&gt; None:\n\"\"\"Initialize adapter for a HDF5 file.\"\"\"\n    self.file: h5py.File = hdf5_file\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.for_path","title":"for_path  <code>classmethod</code>","text":"<pre><code>for_path(dir: Path)\n</code></pre> <p>See dirschema.adapters.IDirectory.for_path.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>@classmethod\ndef for_path(cls, dir: Path):\n\"\"\"See [dirschema.adapters.IDirectory.for_path][].\"\"\"\n    _require_h5py()\n    opened = h5py.File(dir, \"r\")  # auto-closed on GC, no need to do anything\n    return cls(opened)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.get_paths","title":"get_paths","text":"<pre><code>get_paths() -&gt; Iterable[str]\n</code></pre> <p>See dirschema.adapters.IDirectory.get_paths.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def get_paths(self) -&gt; Iterable[str]:\n\"\"\"See [dirschema.adapters.IDirectory.get_paths][].\"\"\"\n    ret = [\"\"]\n    for atr in self.file[\"/\"].attrs.keys():\n        ret.append(f\"{self._ATTR_SEP}{atr}\")\n\n    def collect(name: str) -&gt; None:\n        if name.find(self._ATTR_SEP) &gt;= 0:\n            raise ValueError(f\"Invalid name, must not contain {self._ATTR_SEP}!\")\n        ret.append(name)\n        for atr in self.file[name].attrs.keys():\n            ret.append(f\"{name}{self._ATTR_SEP}{atr}\")\n\n    self.file.visit(collect)\n    return ret\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.is_dir","title":"is_dir","text":"<pre><code>is_dir(path: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_dir.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_dir(self, path: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_dir][].\"\"\"\n    if path == \"\":\n        return True  # root directory\n    if path.find(self._ATTR_SEP) &gt;= 0 or path not in self.file:\n        return False  # not existing or is an attribute\n    if isinstance(self.file[path], h5py.Group):\n        return True  # is a group\n    return False  # something that exists, but is not a group\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.is_file","title":"is_file","text":"<pre><code>is_file(path: str) -&gt; bool\n</code></pre> <p>See dirschema.adapters.IDirectory.is_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def is_file(self, path: str) -&gt; bool:\n\"\"\"See [dirschema.adapters.IDirectory.is_file][].\"\"\"\n    # attributes (treated like special files) exist\n    # if underlying group/dataset exists\n    if path.find(self._ATTR_SEP) &gt;= 0:\n        p = path.split(self._ATTR_SEP)\n        p[0] = p[0] or \"/\"\n        return p[0] in self.file and p[1] in self.file[p[0]].attrs\n    else:\n        # otherwise check it is a dataset (= \"file\")\n        return path in self.file and isinstance(self.file[path], h5py.Dataset)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.decode_json","title":"decode_json","text":"<pre><code>decode_json(data: IO[bytes], path: str) -&gt; Optional[Any]\n</code></pre> <p>See dirschema.adapters.IDirectory.decode_json.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def decode_json(self, data: IO[bytes], path: str) -&gt; Optional[Any]:\n\"\"\"See [dirschema.adapters.IDirectory.decode_json][].\"\"\"\n    bs = data.read()\n    try:\n        ret = json.loads(bs)\n    except json.JSONDecodeError:\n        return None\n\n    if isinstance(ret, dict) and not path.endswith(self._JSON_SUF):\n        return bs\n    else:\n        return ret\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.H5Dir.open_file","title":"open_file","text":"<pre><code>open_file(path: str) -&gt; Optional[IO[bytes]]\n</code></pre> <p>See dirschema.adapters.IDirectory.open_file.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def open_file(self, path: str) -&gt; Optional[IO[bytes]]:\n\"\"\"See [dirschema.adapters.IDirectory.open_file][].\"\"\"\n    p = path\n    if p.find(self._ATTR_SEP) &gt;= 0:\n        # try treating as attribute, return data if it is a string\n        f, s = p.split(self._ATTR_SEP)\n        f = f or \"/\"\n        if f in self.file and s in self.file[f].attrs:\n            dat = self.file[f].attrs[s]\n            if isinstance(dat, h5py.Empty):\n                return None\n            if isinstance(dat, str):\n                if not path.endswith(self._JSON_SUF):\n                    dat = f'\"{dat}\"'  # JSON-encoded string\n            else:\n                dat = json.dumps(dat.tolist())\n            return io.BytesIO(dat.encode(\"utf-8\"))\n        else:\n            return None\n\n    # check that the path exists and is a dataset, but not a numpy array\n    if p not in self.file:\n        return None\n\n    dat = self.file[p]\n    if not isinstance(dat, h5py.Dataset):\n        return None\n\n    bs: Any = dat[()]\n    if isinstance(bs, numpy.ndarray):\n        return None\n\n    # the only kinds of datasets we accept are essentially utf-8 strings\n    # which are represented as possibly wrapped bytes\n    if isinstance(bs, numpy.void):  # void-wrapped bytes -&gt; unpack\n        bs = bs.tobytes()\n\n    return io.BytesIO(bs)\n</code></pre>"},{"location":"reference/dirschema/adapters/#dirschema.adapters.get_adapter_for","title":"get_adapter_for","text":"<pre><code>get_adapter_for(path: Path) -&gt; IDirectory\n</code></pre> <p>Return suitable interface adapter based on path and file extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to directory or archive file</p> required <p>Returns:</p> Type Description <code>IDirectory</code> <p>An adapter instance that can be used for dirschema validation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no suitable adapter was found for the path.</p> Source code in <code>src/dirschema/adapters.py</code> <pre><code>def get_adapter_for(path: Path) -&gt; IDirectory:\n\"\"\"Return suitable interface adapter based on path and file extension.\n\n    Args:\n        path: Path to directory or archive file\n\n    Returns:\n        An adapter instance that can be used for dirschema validation.\n\n    Raises:\n        ValueError: If no suitable adapter was found for the path.\n    \"\"\"\n    if path.is_dir():\n        return RealDir.for_path(path)\n\n    if path.is_file():\n        if path.name.endswith(\"zip\"):\n            return ZipDir.for_path(path)\n        elif path.name.endswith((\"h5\", \"hdf5\")):\n            _require_h5py()\n            return H5Dir.for_path(path)\n\n    raise ValueError(f\"Found no suitable dirschema adapter for path: {path}\")\n</code></pre>"},{"location":"reference/dirschema/cli/","title":"cli","text":"<p>CLI interface of dirschema (see <code>dirschema --help</code>).</p>"},{"location":"reference/dirschema/cli/#dirschema.cli.run_dirschema","title":"run_dirschema","text":"<pre><code>run_dirschema(\n    schema: Path = _schema_arg,\n    dir: Path = _dir_arg,\n    conv: Tuple[str, str, str, str] = _conv_opt,\n    local_basedir: Path = _local_basedir_opt,\n    relative_prefix: str = _rel_prefix_opt,\n    verbose: int = _verbose_opt,\n) -&gt; None\n</code></pre> <p>Run dirschema validation of a directory against a schema.</p> <p>Performs validation according to schema and prints all unsatisfied constraints.</p> Source code in <code>src/dirschema/cli.py</code> <pre><code>@app.command()\ndef run_dirschema(\n    schema: Path = _schema_arg,\n    dir: Path = _dir_arg,\n    conv: Tuple[str, str, str, str] = _conv_opt,\n    local_basedir: Path = _local_basedir_opt,\n    relative_prefix: str = _rel_prefix_opt,\n    verbose: int = _verbose_opt,\n) -&gt; None:\n\"\"\"Run dirschema validation of a directory against a schema.\n\n    Performs validation according to schema and prints all unsatisfied constraints.\n    \"\"\"\n    logger.setLevel(log_level[verbose])\n    local_basedir = local_basedir or schema.parent\n    dsv = DSValidator(\n        schema,\n        MetaConvention.from_tuple(*conv),\n        local_basedir=local_basedir,\n        relative_prefix=relative_prefix,\n    )\n    if errors := dsv.validate(dir):\n        logger.debug(f\"Validation of '{dir}' failed\")\n        dsv.format_errors(errors, sys.stdout)\n        raise typer.Exit(code=1)\n    logger.debug(f\"Validation of '{dir}' successful\")\n</code></pre>"},{"location":"reference/dirschema/core/","title":"core","text":"<p>Core types of dirschema.</p>"},{"location":"reference/dirschema/core/#dirschema.core.DEF_MATCH","title":"DEF_MATCH  <code>module-attribute</code>","text":"<pre><code>DEF_MATCH: Final[str] = '(.*)'\n</code></pre> <p>Default match regex to assume when none is set, but required by semantics.</p>"},{"location":"reference/dirschema/core/#dirschema.core.DEF_REWRITE","title":"DEF_REWRITE  <code>module-attribute</code>","text":"<pre><code>DEF_REWRITE: Final[str] = '\\\\1'\n</code></pre> <p>Default rewrite rule to assume when none is set, but required by semantics.</p>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention","title":"MetaConvention","text":"<p>             Bases: <code>BaseModel</code></p> <p>Filename convention for metadata files that are associated with other entities.</p> <p>It defines where to look for metadata for files that are not themselves known as json, or metadata concerning directories.</p> <p>At the same time, these files are ignored by themselves and act as \"sidecar\" files.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class MetaConvention(BaseModel):\n\"\"\"Filename convention for metadata files that are associated with other entities.\n\n    It defines where to look for metadata for files that are not themselves known\n    as json, or metadata concerning directories.\n\n    At the same time, these files are ignored by themselves and act as \"sidecar\" files.\n    \"\"\"\n\n    pathPrefix: str = \"\"\n    pathSuffix: str = \"\"\n    filePrefix: str = \"\"\n    fileSuffix: str = \"_meta.json\"\n\n    @root_validator\n    def check_valid(cls, values):\n\"\"\"Check that at least one filename extension is non-empty.\"\"\"\n        file_pref_or_suf = values.get(\"filePrefix\", \"\") or values.get(\"fileSuffix\", \"\")\n        if not file_pref_or_suf:\n            raise ValueError(\"At least one of filePrefix or fileSuffix must be set!\")\n        return values\n\n    def to_tuple(self) -&gt; Tuple[str, str, str, str]:\n\"\"\"Convert convention instance to tuple (e.g. used within CLI).\"\"\"\n        return (self.pathPrefix, self.pathSuffix, self.filePrefix, self.fileSuffix)\n\n    @classmethod\n    def from_tuple(cls, pp: str, ps: str, fp: str, fs: str):\n\"\"\"Return new metadata file convention.\"\"\"\n        return MetaConvention(\n            pathPrefix=pp, pathSuffix=ps, filePrefix=fp, fileSuffix=fs\n        )\n\n    def is_meta(self, path: str) -&gt; bool:\n\"\"\"Check whether given path is a metadata file according to the convention.\"\"\"\n        prts = Path(path).parts\n        if len(prts) == 0:  # root dir\n            return False\n        if self.filePrefix != \"\" and not prts[-1].startswith(self.filePrefix):\n            return False\n        if self.fileSuffix != \"\" and not prts[-1].endswith(self.fileSuffix):\n            return False\n        pieces = int(self.pathPrefix != \"\") + int(self.pathSuffix != \"\")\n        if len(prts) &lt; 1 + pieces:\n            return False\n        pp = self.pathPrefix == \"\" or prts[0] == self.pathPrefix\n        ps = self.pathSuffix == \"\" or prts[-2] == self.pathSuffix\n        return pp and ps\n\n    def meta_for(self, path: str, is_dir: bool = False) -&gt; str:\n\"\"\"Return metadata filename for provided path, based on this convention.\"\"\"\n        ps = list(Path(path).parts)\n        newp = []\n\n        if self.pathPrefix != \"\":\n            newp.append(self.pathPrefix)\n        newp += ps[:-1]\n        if not is_dir and self.pathSuffix != \"\":\n            newp.append(self.pathSuffix)\n        name = ps[-1] if len(ps) &gt; 0 else \"\"\n\n        if is_dir:\n            newp.append(name)\n            if self.pathSuffix != \"\":\n                newp.append(self.pathSuffix)\n            metaname = self.filePrefix + self.fileSuffix\n            newp.append(metaname)\n        else:\n            metaname = self.filePrefix + name + self.fileSuffix\n            newp.append(metaname)\n        return str(Path().joinpath(*newp))\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention.check_valid","title":"check_valid","text":"<pre><code>check_valid(values)\n</code></pre> <p>Check that at least one filename extension is non-empty.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>@root_validator\ndef check_valid(cls, values):\n\"\"\"Check that at least one filename extension is non-empty.\"\"\"\n    file_pref_or_suf = values.get(\"filePrefix\", \"\") or values.get(\"fileSuffix\", \"\")\n    if not file_pref_or_suf:\n        raise ValueError(\"At least one of filePrefix or fileSuffix must be set!\")\n    return values\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention.to_tuple","title":"to_tuple","text":"<pre><code>to_tuple() -&gt; Tuple[str, str, str, str]\n</code></pre> <p>Convert convention instance to tuple (e.g. used within CLI).</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def to_tuple(self) -&gt; Tuple[str, str, str, str]:\n\"\"\"Convert convention instance to tuple (e.g. used within CLI).\"\"\"\n    return (self.pathPrefix, self.pathSuffix, self.filePrefix, self.fileSuffix)\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention.from_tuple","title":"from_tuple  <code>classmethod</code>","text":"<pre><code>from_tuple(pp: str, ps: str, fp: str, fs: str)\n</code></pre> <p>Return new metadata file convention.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>@classmethod\ndef from_tuple(cls, pp: str, ps: str, fp: str, fs: str):\n\"\"\"Return new metadata file convention.\"\"\"\n    return MetaConvention(\n        pathPrefix=pp, pathSuffix=ps, filePrefix=fp, fileSuffix=fs\n    )\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention.is_meta","title":"is_meta","text":"<pre><code>is_meta(path: str) -&gt; bool\n</code></pre> <p>Check whether given path is a metadata file according to the convention.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def is_meta(self, path: str) -&gt; bool:\n\"\"\"Check whether given path is a metadata file according to the convention.\"\"\"\n    prts = Path(path).parts\n    if len(prts) == 0:  # root dir\n        return False\n    if self.filePrefix != \"\" and not prts[-1].startswith(self.filePrefix):\n        return False\n    if self.fileSuffix != \"\" and not prts[-1].endswith(self.fileSuffix):\n        return False\n    pieces = int(self.pathPrefix != \"\") + int(self.pathSuffix != \"\")\n    if len(prts) &lt; 1 + pieces:\n        return False\n    pp = self.pathPrefix == \"\" or prts[0] == self.pathPrefix\n    ps = self.pathSuffix == \"\" or prts[-2] == self.pathSuffix\n    return pp and ps\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.MetaConvention.meta_for","title":"meta_for","text":"<pre><code>meta_for(path: str, is_dir: bool = False) -&gt; str\n</code></pre> <p>Return metadata filename for provided path, based on this convention.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def meta_for(self, path: str, is_dir: bool = False) -&gt; str:\n\"\"\"Return metadata filename for provided path, based on this convention.\"\"\"\n    ps = list(Path(path).parts)\n    newp = []\n\n    if self.pathPrefix != \"\":\n        newp.append(self.pathPrefix)\n    newp += ps[:-1]\n    if not is_dir and self.pathSuffix != \"\":\n        newp.append(self.pathSuffix)\n    name = ps[-1] if len(ps) &gt; 0 else \"\"\n\n    if is_dir:\n        newp.append(name)\n        if self.pathSuffix != \"\":\n            newp.append(self.pathSuffix)\n        metaname = self.filePrefix + self.fileSuffix\n        newp.append(metaname)\n    else:\n        metaname = self.filePrefix + name + self.fileSuffix\n        newp.append(metaname)\n    return str(Path().joinpath(*newp))\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.PathSlice","title":"PathSlice","text":"<p>             Bases: <code>BaseModel</code></p> <p>Helper class to slice into path segments and do regex-based match/substitution.</p> <p>Invariant: into(path, sl).unslice() == path for all sl and path.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class PathSlice(BaseModel):\n\"\"\"Helper class to slice into path segments and do regex-based match/substitution.\n\n    Invariant: into(path, sl).unslice() == path for all sl and path.\n    \"\"\"\n\n    slicePre: Optional[str]\n    sliceStr: str\n    sliceSuf: Optional[str]\n\n    @classmethod\n    def into(\n        cls, path: str, start: Optional[int] = None, stop: Optional[int] = None\n    ) -&gt; PathSlice:\n\"\"\"Slice into a path, splitting on the slashes.\n\n        Slice semantics is mostly like Python, except that stop=0 means\n        \"until the end\", so that [0:0] means the full path.\n        \"\"\"\n        segs = path.split(\"/\")\n        pref = \"/\".join(segs[: start if start else 0])\n        inner = \"/\".join(segs[start : stop if stop != 0 else None])  # noqa: E203\n        suf = \"/\".join(segs[stop:] if stop else [])\n        return PathSlice(\n            slicePre=pref if pref else None,\n            sliceStr=inner,\n            sliceSuf=suf if suf else None,\n        )\n\n    def unslice(self) -&gt; str:\n\"\"\"Inverse of slice operation (recovers complete path string).\"\"\"\n        return \"/\".join([x for x in [self.slicePre, self.sliceStr, self.sliceSuf] if x])\n\n    _def_pat = re.compile(\"(.*)\")\n\"\"\"Default pattern (match anything, put into capture group).\"\"\"\n\n    def match(self, pat: Optional[Union[re.Pattern, str]] = None):\n\"\"\"Do full regex match on current slice.\"\"\"\n        pat = pat or self._def_pat\n        if isinstance(pat, str):\n            pat = re.compile(pat)\n        return pat.fullmatch(self.sliceStr)\n\n    def rewrite(\n        self, pat: Optional[Union[re.Pattern, str]] = None, sub: Optional[str] = None\n    ) -&gt; Optional[PathSlice]:\n\"\"\"Match and rewrite in the slice string and return a new PathSlice.\n\n        If no pattern given, default pattern is used.\n        If no substitution is given, just match on pattern is performed.\n        Returns new PathSlice with possibly rewritten slice.\n        Returns None if match fails.\n        Raises exception of rewriting fails due to e.g. invalid capture groups.\n        \"\"\"\n        if m := self.match(pat):\n            ret = self.copy()\n            if sub is not None:\n                ret.sliceStr = m.expand(sub)\n            return ret\n        return None\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.PathSlice.into","title":"into  <code>classmethod</code>","text":"<pre><code>into(\n    path: str,\n    start: Optional[int] = None,\n    stop: Optional[int] = None,\n) -&gt; PathSlice\n</code></pre> <p>Slice into a path, splitting on the slashes.</p> <p>Slice semantics is mostly like Python, except that stop=0 means \"until the end\", so that [0:0] means the full path.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>@classmethod\ndef into(\n    cls, path: str, start: Optional[int] = None, stop: Optional[int] = None\n) -&gt; PathSlice:\n\"\"\"Slice into a path, splitting on the slashes.\n\n    Slice semantics is mostly like Python, except that stop=0 means\n    \"until the end\", so that [0:0] means the full path.\n    \"\"\"\n    segs = path.split(\"/\")\n    pref = \"/\".join(segs[: start if start else 0])\n    inner = \"/\".join(segs[start : stop if stop != 0 else None])  # noqa: E203\n    suf = \"/\".join(segs[stop:] if stop else [])\n    return PathSlice(\n        slicePre=pref if pref else None,\n        sliceStr=inner,\n        sliceSuf=suf if suf else None,\n    )\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.PathSlice.unslice","title":"unslice","text":"<pre><code>unslice() -&gt; str\n</code></pre> <p>Inverse of slice operation (recovers complete path string).</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def unslice(self) -&gt; str:\n\"\"\"Inverse of slice operation (recovers complete path string).\"\"\"\n    return \"/\".join([x for x in [self.slicePre, self.sliceStr, self.sliceSuf] if x])\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.PathSlice.match","title":"match","text":"<pre><code>match(pat: Optional[Union[re.Pattern, str]] = None)\n</code></pre> <p>Do full regex match on current slice.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def match(self, pat: Optional[Union[re.Pattern, str]] = None):\n\"\"\"Do full regex match on current slice.\"\"\"\n    pat = pat or self._def_pat\n    if isinstance(pat, str):\n        pat = re.compile(pat)\n    return pat.fullmatch(self.sliceStr)\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.PathSlice.rewrite","title":"rewrite","text":"<pre><code>rewrite(\n    pat: Optional[Union[re.Pattern, str]] = None,\n    sub: Optional[str] = None,\n) -&gt; Optional[PathSlice]\n</code></pre> <p>Match and rewrite in the slice string and return a new PathSlice.</p> <p>If no pattern given, default pattern is used. If no substitution is given, just match on pattern is performed. Returns new PathSlice with possibly rewritten slice. Returns None if match fails. Raises exception of rewriting fails due to e.g. invalid capture groups.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def rewrite(\n    self, pat: Optional[Union[re.Pattern, str]] = None, sub: Optional[str] = None\n) -&gt; Optional[PathSlice]:\n\"\"\"Match and rewrite in the slice string and return a new PathSlice.\n\n    If no pattern given, default pattern is used.\n    If no substitution is given, just match on pattern is performed.\n    Returns new PathSlice with possibly rewritten slice.\n    Returns None if match fails.\n    Raises exception of rewriting fails due to e.g. invalid capture groups.\n    \"\"\"\n    if m := self.match(pat):\n        ret = self.copy()\n        if sub is not None:\n            ret.sliceStr = m.expand(sub)\n        return ret\n    return None\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.JSONSchema","title":"JSONSchema","text":"<p>             Bases: <code>BaseModel</code></p> <p>Helper class wrapping an arbitrary JSON Schema to be acceptable for pydantic.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class JSONSchema(BaseModel):\n\"\"\"Helper class wrapping an arbitrary JSON Schema to be acceptable for pydantic.\"\"\"\n\n    @classmethod\n    def __get_validators__(cls):  # noqa: D105\n        yield cls.validate\n\n    @classmethod\n    def validate(cls, v):  # noqa: D102\n        Draft202012Validator.check_schema(v)  # throws SchemaError if schema is invalid\n        return v\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.TypeEnum","title":"TypeEnum","text":"<p>             Bases: <code>Enum</code></p> <p>Possible values for a path type inside a dirschema rule.</p> <p>MISSING means that the path must not exist (i.e. neither file or directory), whereas ANY means that any of these options is fine, as long as the path exists.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class TypeEnum(Enum):\n\"\"\"Possible values for a path type inside a dirschema rule.\n\n    MISSING means that the path must not exist (i.e. neither file or directory),\n    whereas ANY means that any of these options is fine, as long as the path exists.\n    \"\"\"\n\n    MISSING = False\n    FILE = \"file\"\n    DIR = \"dir\"\n    ANY = True\n\n    def is_satisfied(self, is_file: bool, is_dir: bool) -&gt; bool:\n\"\"\"Check whether the flags of a path satisfy this path type.\"\"\"\n        if self == TypeEnum.MISSING and (is_file or is_dir):\n            return False\n        if self == TypeEnum.ANY and not (is_file or is_dir):\n            return False\n        if self == TypeEnum.DIR and not is_dir:\n            return False\n        if self == TypeEnum.FILE and not is_file:\n            return False\n        return True\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.TypeEnum.is_satisfied","title":"is_satisfied","text":"<pre><code>is_satisfied(is_file: bool, is_dir: bool) -&gt; bool\n</code></pre> <p>Check whether the flags of a path satisfy this path type.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def is_satisfied(self, is_file: bool, is_dir: bool) -&gt; bool:\n\"\"\"Check whether the flags of a path satisfy this path type.\"\"\"\n    if self == TypeEnum.MISSING and (is_file or is_dir):\n        return False\n    if self == TypeEnum.ANY and not (is_file or is_dir):\n        return False\n    if self == TypeEnum.DIR and not is_dir:\n        return False\n    if self == TypeEnum.FILE and not is_file:\n        return False\n    return True\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.DSRule","title":"DSRule","text":"<p>             Bases: <code>BaseModel</code></p> <p>A DirSchema rule is either a trivial (boolean) rule, or a complex object.</p> <p>Use this class for parsing, if it is not known which of these it is.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class DSRule(BaseModel):\n\"\"\"A DirSchema rule is either a trivial (boolean) rule, or a complex object.\n\n    Use this class for parsing, if it is not known which of these it is.\n    \"\"\"\n\n    __root__: Union[bool, Rule]\n\n    def __init__(self, b: Optional[bool] = None, **kwargs):\n\"\"\"Construct wrapped boolean or object, depending on arguments.\"\"\"\n        if b is not None:\n            return super().__init__(__root__=b)\n        elif \"__root__\" in kwargs:\n            if len(kwargs) != 1:\n                raise ValueError(\"No extra kwargs may be passed with __root__!\")\n            return super().__init__(**kwargs)\n        else:\n            return super().__init__(__root__=Rule(**kwargs))\n\n    def __repr__(self) -&gt; str:\n\"\"\"Make wrapper transparent and just return repr of wrapped object.\"\"\"\n        if isinstance(self.__root__, bool):\n            return \"true\" if self.__root__ else \"false\"\n        else:\n            return repr(self.__root__)\n\n    def __bool__(self):\n\"\"\"Just return value for wrapped object.\"\"\"\n        return bool(self.__root__)\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.DSRule.__init__","title":"__init__","text":"<pre><code>__init__(\n    b: Optional[bool] = None, **kwargs: Optional[bool]\n)\n</code></pre> <p>Construct wrapped boolean or object, depending on arguments.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def __init__(self, b: Optional[bool] = None, **kwargs):\n\"\"\"Construct wrapped boolean or object, depending on arguments.\"\"\"\n    if b is not None:\n        return super().__init__(__root__=b)\n    elif \"__root__\" in kwargs:\n        if len(kwargs) != 1:\n            raise ValueError(\"No extra kwargs may be passed with __root__!\")\n        return super().__init__(**kwargs)\n    else:\n        return super().__init__(__root__=Rule(**kwargs))\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.DSRule.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>Make wrapper transparent and just return repr of wrapped object.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def __repr__(self) -&gt; str:\n\"\"\"Make wrapper transparent and just return repr of wrapped object.\"\"\"\n    if isinstance(self.__root__, bool):\n        return \"true\" if self.__root__ else \"false\"\n    else:\n        return repr(self.__root__)\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.DSRule.__bool__","title":"__bool__","text":"<pre><code>__bool__()\n</code></pre> <p>Just return value for wrapped object.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def __bool__(self):\n\"\"\"Just return value for wrapped object.\"\"\"\n    return bool(self.__root__)\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.Rule","title":"Rule","text":"<p>             Bases: <code>BaseModel</code></p> <p>A DirSchema is a conjunction of a subset of distinct constraints/keywords.</p> Source code in <code>src/dirschema/core.py</code> <pre><code>class Rule(BaseModel):\n\"\"\"A DirSchema is a conjunction of a subset of distinct constraints/keywords.\"\"\"\n\n    # primitive:\n    type: Optional[TypeEnum] = Field(\n        description=\"Check that path is a file / is a dir.\"\n    )\n\n    valid: Optional[Union[JSONSchema, str]] = Field(\n        description=\"Validate file against provided schema or validator.\"\n    )\n\n    # this will use the provided metadataConvention for rewriting to the right path\n    validMeta: Optional[Union[JSONSchema, str]] = Field(\n        description=\"Validate external metadata against provided schema or validator.\"\n    )\n\n    # these are JSON-Schema-like logical operators:\n\n    allOf: List[DSRule] = Field([], description=\"Conjunction (evaluated in order).\")\n\n    anyOf: List[DSRule] = Field([], description=\"Disjunction (evaluated in order).\")\n\n    oneOf: List[DSRule] = Field([], description=\"Exact-1-of-N (evaluated in order).\")\n\n    not_: Optional[DSRule] = Field(description=\"Negation of a rule.\", alias=\"not\")\n\n    # introduced for better error reporting (will yield no error message on failure)\n    # So this is more for dirschema \"control-flow\"\n    if_: Optional[DSRule] = Field(\n        description=\"Depending on result of rule, will proceed with 'then' or 'else'.\",\n        alias=\"if\",\n    )\n\n    then: Optional[DSRule] = Field(\n        description=\"Evaluated if 'if' rule exists and satisfied.\",\n    )\n\n    else_: Optional[DSRule] = Field(\n        description=\"Evaluated if 'if' rule exists and not satisfied.\",\n        alias=\"else\",\n    )\n\n    # match and rewrite (path inspection and manipulation):\n\n    # we keep the total match and the capture groups for possible rewriting\n    # the match data + start/end is also inherited down into children\n    match: Optional[Pattern] = Field(\n        description=\"Path must match. Sets capture groups.\"\n    )\n\n    # indices of path segments (i.e. array parts after splitting on /)\n    # matchStart &lt; matchEnd (unless start pos. and end neg.)\n    # matchStart = -1 = match only in final segment\n    # it's python slice without 'step' option\n    # this means, missing segments are ignored\n    # to have \"exact\" number of segments, match on a pattern with required # of / first\n    matchStart: Optional[int]\n    matchStop: Optional[int]\n\n    # only do rewrite if match was successful\n    rewrite: Optional[str]\n\n    # if rewrite is set, apply 'next' to rewritten path instead of original\n    # missing rewrite is like rewrite \\1, missing match is like \".*\"\n    next: Optional[DSRule] = Field(\n        description=\"If current rule is satisfied, evaluate the 'next' rule.\"\n    )\n\n    # improve error reporting by making it customizable\n    # overrides all other errors on the level of this rule (but keeps subrule errors)\n    # if set to \"\", will report no error message for this rule\n    description: Optional[str] = Field(\n        None,\n        description=\"Custom error message to be shown to the user if this rule fails.\",\n    )\n    # used to prune noisy error message accumulation to some high level summary\n    details: bool = Field(\n        True,\n        description=\"If set, keep errors from sub-rules, otherwise ignore them.\",\n    )\n\n    # ----\n\n    def __repr__(self, stream=None) -&gt; str:\n\"\"\"Print out the rule as YAML (only the non-default values).\"\"\"\n        res = json.loads(self.json(exclude_defaults=True))\n\n        if not stream:\n            stream = io.StringIO()\n            yaml.dump(res, stream)\n            return stream.getvalue().strip()\n\n        yaml.dump(res, stream)\n        return \"\"\n\n    class Config:  # noqa: D106\n        extra = Extra.forbid\n</code></pre>"},{"location":"reference/dirschema/core/#dirschema.core.Rule.__repr__","title":"__repr__","text":"<pre><code>__repr__(stream = None) -&gt; str\n</code></pre> <p>Print out the rule as YAML (only the non-default values).</p> Source code in <code>src/dirschema/core.py</code> <pre><code>def __repr__(self, stream=None) -&gt; str:\n\"\"\"Print out the rule as YAML (only the non-default values).\"\"\"\n    res = json.loads(self.json(exclude_defaults=True))\n\n    if not stream:\n        stream = io.StringIO()\n        yaml.dump(res, stream)\n        return stream.getvalue().strip()\n\n    yaml.dump(res, stream)\n    return \"\"\n</code></pre>"},{"location":"reference/dirschema/log/","title":"log","text":"<p>Setup of logging for dirschema.</p>"},{"location":"reference/dirschema/validate/","title":"validate","text":"<p>Validation API functionality for DirSchema.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidationErrors","title":"DSValidationErrors  <code>module-attribute</code>","text":"<pre><code>DSValidationErrors = Dict[\n    Tuple[Union[str, int], ...], DSValidationError\n]\n</code></pre> <p>Dict mapping from error locations in schema to errors.</p> <p>The keys of this dict can be used to access the corresponding sub-rule if the schema is loaded as a JSON dict.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidationResult","title":"DSValidationResult  <code>module-attribute</code>","text":"<pre><code>DSValidationResult = Dict[str, DSValidationErrors]\n</code></pre> <p>The validation result is a mapping from file/directory paths to corresponding validation errors for all entities where validation failed.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidationError","title":"DSValidationError","text":"<p>             Bases: <code>BaseModel</code></p> <p>A single Dirschema validation error.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>class DSValidationError(BaseModel):\n\"\"\"A single Dirschema validation error.\"\"\"\n\n    path: str\n\"\"\"File path that was evaluated (possibly a result of applied rewrites).\"\"\"\n\n    err: Union[str, JSONValidationErrors]\n\"\"\"Error object (error message or a dict with refined validation errors).\"\"\"\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidationError.path","title":"path  <code>instance-attribute</code>","text":"<pre><code>path: str\n</code></pre> <p>File path that was evaluated (possibly a result of applied rewrites).</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidationError.err","title":"err  <code>instance-attribute</code>","text":"<pre><code>err: Union[str, JSONValidationErrors]\n</code></pre> <p>Error object (error message or a dict with refined validation errors).</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx","title":"DSEvalCtx","text":"<p>             Bases: <code>BaseModel</code></p> <p>DirSchema evaluation context, used like a Reader Monad.</p> <p>Contains information that is required to evaluate a rule for a path.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>class DSEvalCtx(BaseModel):\n\"\"\"DirSchema evaluation context, used like a Reader Monad.\n\n    Contains information that is required to evaluate a rule for a path.\n    \"\"\"\n\n    class Config:  # noqa: D106\n        arbitrary_types_allowed = True\n\n    dirAdapter: IDirectory\n\"\"\"Adapter to access metadata files and get paths from.\"\"\"\n\n    metaConvention: MetaConvention = MetaConvention()\n\"\"\"Convention to use for validMeta.\"\"\"\n\n    # ----\n\n    errors: DSValidationErrors = {}\n    failed: bool = False\n\n    filePath: str = \"\"\n\"\"\"Path of currently checked file (possibly rewritten).\"\"\"\n\n    location: List[Union[str, int]] = []\n\"\"\"Relative location of current rule.\"\"\"\n\n    # passed down from parent rule / overridden with current rule:\n\n    matchStart: int = 0\n    matchStop: int = 0\n    matchPat: Optional[re.Pattern] = None\n\n    @classmethod\n    def fresh(cls, rule: DSRule, **kwargs):\n\"\"\"Initialize a fresh evaluation context.\"\"\"\n        ret = DSEvalCtx(**kwargs)  # initialize most fields from passed kwargs\n        return ret.descend(rule)  # initialize match* fields from rule\n\n    def descend(\n        self,\n        rule: DSRule,\n        filepath: Optional[str] = None,\n        reachedVia: Optional[Any] = None,\n    ) -&gt; DSEvalCtx:\n\"\"\"Return a new context updated with fields from the given rule.\n\n        Input must be the next sub-rule, the possibly rewritten entity path\n        and the key in the parent rule that is used to access the sub-rule.\n\n        This will not preserve the parent errors (use `add_errors` to merge).\n        \"\"\"\n        ret = self.copy()\n        ret.errors = {}\n        ret.location = list(self.location)\n\n        if isinstance(rule.__root__, Rule):\n            # override match configuration and pattern, if specified in child rule\n            rl: Rule = rule.__root__\n            if rl.matchStart:\n                ret.matchStart = rl.matchStart\n            if rl.matchStop:\n                ret.matchStart = rl.matchStop\n            if rl.match:\n                ret.matchPat = rl.match\n\n        if filepath is not None:\n            ret.filePath = filepath\n\n        if reachedVia is not None:\n            ret.location.append(reachedVia)\n\n        return ret\n\n    def add_error(\n        self,\n        err: Any,\n        child: Optional[Union[str, int]] = None,\n        path: Optional[str] = None,\n    ):\n\"\"\"Add an error object at current location.\n\n        Will extend current location with `child`, if given,\n        will use passed `path`, if given.\n        \"\"\"\n        loc = self.location if child is None else self.location + [child]\n        fp = path or self.filePath\n        self.errors[tuple(loc)] = DSValidationError(path=fp, err=err)\n\n    def add_errors(self, *err_dicts):\n\"\"\"Merge all passed error dicts into the errors of this context.\"\"\"\n        for err_dict in err_dicts:\n            self.errors.update(err_dict)\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.dirAdapter","title":"dirAdapter  <code>instance-attribute</code>","text":"<pre><code>dirAdapter: IDirectory\n</code></pre> <p>Adapter to access metadata files and get paths from.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.metaConvention","title":"metaConvention  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>metaConvention: MetaConvention = MetaConvention()\n</code></pre> <p>Convention to use for validMeta.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.filePath","title":"filePath  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>filePath: str = ''\n</code></pre> <p>Path of currently checked file (possibly rewritten).</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.location","title":"location  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>location: List[Union[str, int]] = []\n</code></pre> <p>Relative location of current rule.</p>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.fresh","title":"fresh  <code>classmethod</code>","text":"<pre><code>fresh(rule: DSRule, **kwargs: DSRule)\n</code></pre> <p>Initialize a fresh evaluation context.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>@classmethod\ndef fresh(cls, rule: DSRule, **kwargs):\n\"\"\"Initialize a fresh evaluation context.\"\"\"\n    ret = DSEvalCtx(**kwargs)  # initialize most fields from passed kwargs\n    return ret.descend(rule)  # initialize match* fields from rule\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.descend","title":"descend","text":"<pre><code>descend(\n    rule: DSRule,\n    filepath: Optional[str] = None,\n    reachedVia: Optional[Any] = None,\n) -&gt; DSEvalCtx\n</code></pre> <p>Return a new context updated with fields from the given rule.</p> <p>Input must be the next sub-rule, the possibly rewritten entity path and the key in the parent rule that is used to access the sub-rule.</p> <p>This will not preserve the parent errors (use <code>add_errors</code> to merge).</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def descend(\n    self,\n    rule: DSRule,\n    filepath: Optional[str] = None,\n    reachedVia: Optional[Any] = None,\n) -&gt; DSEvalCtx:\n\"\"\"Return a new context updated with fields from the given rule.\n\n    Input must be the next sub-rule, the possibly rewritten entity path\n    and the key in the parent rule that is used to access the sub-rule.\n\n    This will not preserve the parent errors (use `add_errors` to merge).\n    \"\"\"\n    ret = self.copy()\n    ret.errors = {}\n    ret.location = list(self.location)\n\n    if isinstance(rule.__root__, Rule):\n        # override match configuration and pattern, if specified in child rule\n        rl: Rule = rule.__root__\n        if rl.matchStart:\n            ret.matchStart = rl.matchStart\n        if rl.matchStop:\n            ret.matchStart = rl.matchStop\n        if rl.match:\n            ret.matchPat = rl.match\n\n    if filepath is not None:\n        ret.filePath = filepath\n\n    if reachedVia is not None:\n        ret.location.append(reachedVia)\n\n    return ret\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.add_error","title":"add_error","text":"<pre><code>add_error(\n    err: Any,\n    child: Optional[Union[str, int]] = None,\n    path: Optional[str] = None,\n)\n</code></pre> <p>Add an error object at current location.</p> <p>Will extend current location with <code>child</code>, if given, will use passed <code>path</code>, if given.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def add_error(\n    self,\n    err: Any,\n    child: Optional[Union[str, int]] = None,\n    path: Optional[str] = None,\n):\n\"\"\"Add an error object at current location.\n\n    Will extend current location with `child`, if given,\n    will use passed `path`, if given.\n    \"\"\"\n    loc = self.location if child is None else self.location + [child]\n    fp = path or self.filePath\n    self.errors[tuple(loc)] = DSValidationError(path=fp, err=err)\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSEvalCtx.add_errors","title":"add_errors","text":"<pre><code>add_errors(*err_dicts)\n</code></pre> <p>Merge all passed error dicts into the errors of this context.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def add_errors(self, *err_dicts):\n\"\"\"Merge all passed error dicts into the errors of this context.\"\"\"\n    for err_dict in err_dicts:\n        self.errors.update(err_dict)\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator","title":"DSValidator","text":"<p>Validator class that performs dirschema validation for a given dirschema.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>class DSValidator:\n\"\"\"Validator class that performs dirschema validation for a given dirschema.\"\"\"\n\n    def __init__(\n        self,\n        schema: Union[bool, Rule, DSRule, str, Path],\n        meta_conv: Optional[MetaConvention] = None,\n        local_basedir: Optional[Path] = None,\n        relative_prefix: str = \"\",\n    ) -&gt; None:\n\"\"\"Construct validator instance from given schema or schema location.\n\n        Accepts DSRule, raw bool or Rule, or a str/Path that is interpreted as location.\n        \"\"\"\n        self.meta_conv = meta_conv or MetaConvention()\n        self.local_basedir = local_basedir\n        self.relative_prefix = relative_prefix\n\n        # if the passed relative prefix is a custom plugin, we cannot use this\n        # for $ref resolving, so we will ignore it in the Json/Yaml loader\n        is_plugin_prefix = relative_prefix.find(\"v#\") &lt; relative_prefix.find(\"://\") == 0\n\n        # take care of the passed schema based on its type\n        if isinstance(schema, bool) or isinstance(schema, Rule):\n            self.schema = DSRule(__root__=schema)\n        elif isinstance(schema, DSRule):\n            self.schema = schema\n        elif isinstance(schema, str) or isinstance(schema, Path):\n            uri = to_uri(str(schema), self.local_basedir, self.relative_prefix)\n            dat = load_json(\n                uri,\n                local_basedir=self.local_basedir,\n                relative_prefix=self.relative_prefix if not is_plugin_prefix else \"\",\n            )\n            # use deepcopy to get rid of jsonref (see jsonref issue #9)\n            # otherwise we will get problems with pydantic serialization later\n            self.schema = DSRule.parse_obj(copy.deepcopy(dat))\n        else:\n            raise ValueError(f\"Do not know how to process provided schema: {schema}\")\n\n        logger.debug(\n            \"Initialized dirschema validator\\n\"\n            f\"schema: {self.schema}\\n\"\n            f\"meta_conv: {self.meta_conv}\\n\"\n            f\"local_basedir: {self.local_basedir}\\n\"\n        )\n\n    @classmethod\n    def errors_to_json(cls, errs: DSValidationResult) -&gt; Dict[str, Any]:\n\"\"\"Convert the validation result to a JSON-compatible dict.\n\n        Resulting structure is (file path -&gt; schema location -&gt; error message or dict).\n        \"\"\"\n        return {\n            file_path: {\n                loc_to_jsonpointer(err_loc): json_dict(err_obj, exclude_defaults=True)\n                for err_loc, err_obj in file_errors.items()\n            }\n            for file_path, file_errors in errs.items()\n        }\n\n    @classmethod\n    def format_errors(cls, errs: DSValidationResult, stream=None) -&gt; Optional[str]:\n\"\"\"Report errors as YAML output.\n\n        If a stream is provided, prints it out. Otherwise, returns it as string.\n        \"\"\"\n        of = stream or io.StringIO()\n        yaml.dump(cls.errors_to_json(errs), of)\n        if not stream:\n            return of.getvalue()\n        return None\n\n    def validate(\n        self, root_path: Union[Path, IDirectory], **kwargs\n    ) -&gt; DSValidationResult:\n\"\"\"Validate a directory, return all validation errors (unsatisfied rules).\n\n        If `root_path` is an instance of `IDirectory`, it will be used directly.\n\n        If `root_path` is a `Path`, this function will try to pick the correct\n        interface for interpreting \"files\" and \"directories\", depending on\n        whether the provided file is a directory or a supported kind of archive\n        file with internal structure.\n\n        Depending on the used metadata convention, the companion metadata files\n        matching the convention will be filtered out from the set of validated\n        paths.\n\n        Returns\n            Error dict that is empty in case of success, or otherwise contains\n            for each path with validation errors another dict with the errors.\n        \"\"\"\n        logger.debug(f\"validate '{root_path}' ...\")\n        if isinstance(root_path, Path):\n            root_path = get_adapter_for(root_path)\n        paths = [p for p in root_path.get_paths() if not self.meta_conv.is_meta(p)]\n        errors: Dict[str, Any] = {}\n        # run validation for each filepath, collect errors separately\n        for p in paths:\n            ctx = DSEvalCtx.fresh(\n                self.schema,\n                dirAdapter=root_path,\n                metaConvention=self.meta_conv,\n                filePath=p,\n                **kwargs,\n            )\n            logger.debug(f\"validate_path '{p}' ...\")\n            success = self.validate_path(p, self.schema, ctx)\n            logger.debug(f\"validate_path '{p}' -&gt; {success}\")\n            if not success:\n                errors[p] = ctx.errors or {\n                    (): DSValidationError(\n                        path=p, err=\"Validation failed (no error log available).\"\n                    )\n                }\n        return errors\n\n    def validate_path(self, path: str, rule: DSRule, curCtx: DSEvalCtx) -&gt; bool:\n\"\"\"Apply rule to path of file/directory under given evaluation context.\n\n        Will collect errors in the context object.\n\n        Note that not all errors might be reported, as the sub-rules are\n        evaluated in different stages and each stage aborts evaluation on\n        failure (i.e. match/rewrite, primitive rules, complex logic rules,\n        `next` sub-rule)\n\n        Returns True iff validation of this rule was successful.\n        \"\"\"\n        logger.debug(f\"validate_path '{path}', at rule location: {curCtx.location}\")\n\n        # special case: trivial bool rule\n        if isinstance(rule.__root__, bool):\n            logger.debug(curCtx.location, \"trivial rule\")\n            if not rule.__root__:\n                curCtx.failed = True\n                curCtx.add_error(\"Reached unsatisfiable 'false' rule\")\n            return not curCtx.failed\n\n        rl = rule.__root__  # unpack rule\n        # assert isinstance(rl, Rule)\n\n        # 1. match / rewrite\n        # if rewrite is set, don't need to do separate match,just try rewriting\n        # match/rewrite does not produce an error on its own, but can fail\n        # because \"match failure\" is usually not \"validation failure\"\n        psl = PathSlice.into(path, curCtx.matchStart, curCtx.matchStop)\n        nextPath: str = path  # to be used for implication later on\n        if rl.match or rl.rewrite:\n            # important! using the match pattern from the context (could be inherited)\n            rewritten = psl.rewrite(curCtx.matchPat, rl.rewrite)\n            if rewritten is not None:\n                nextPath = rewritten.unslice()\n            else:  # failed match or rewrite\n                op = \"rewrite\" if rl.rewrite else \"match\"\n                pat = curCtx.matchPat or psl._def_pat\n                matchPat = f\"match '{pat.pattern}'\"\n                rwPat = f\" and rewrite to '{str(rl.rewrite)}'\" if rl.rewrite else \"\"\n\n                if rl.description:  # add custom error without expanding groups\n                    curCtx.add_error(rl.description, op)\n                else:\n                    curCtx.add_error(f\"Failed to {matchPat}{rwPat}\", op)\n                curCtx.failed = True\n                return False\n\n        # 2. proceed with the other primitive constraints\n\n        def add_error(*args):\n\"\"\"If desc is set, add desc error once and else add passed error.\"\"\"\n            if rl.description is None:\n                curCtx.add_error(*args)\n            elif rl.description != \"\" and not curCtx.failed:\n                # add error with expanded groups for better error messages\n                curCtx.add_error(psl.match(curCtx.matchPat).expand(rl.description))\n            curCtx.failed = True\n\n        # take care of type constraint\n        is_file = curCtx.dirAdapter.is_file(path)\n        is_dir = curCtx.dirAdapter.is_dir(path)\n        if rl.type is not None and not rl.type.is_satisfied(is_file, is_dir):\n            msg = f\"Entity does not have expected type: '{rl.type.value}'\"\n            if rl.type == TypeEnum.ANY:\n                msg = \"Entity must exist (type: true)\"\n            elif rl.type == TypeEnum.MISSING:\n                msg = \"Entity must not exist (type: false)\"\n            add_error(msg, \"type\", None)\n\n        # take care of metadata JSON Schema validation constraint\n        for key in (\"valid\", \"validMeta\"):\n            if rl.__dict__[key] is None:  # attribute not set\n                continue\n\n            if not is_file and not is_dir:\n                add_error(f\"Path '{path}' does not exist\", key, None)\n                continue\n\n            # use metadata convention for validMeta\n            metapath = path\n            if key == \"validMeta\":\n                metapath = curCtx.metaConvention.meta_for(path, is_dir=is_dir)\n\n            # load metadata file\n            dat = curCtx.dirAdapter.open_file(metapath)\n            if dat is None:\n                add_error(f\"File '{metapath}' could not be loaded\", key, metapath)\n                continue\n\n            # prepare correct validation method (JSON Schema or custom plugin)\n            schema_or_plugin = resolve_validator(\n                rl.__dict__[key],\n                local_basedir=self.local_basedir,\n                relative_prefix=self.relative_prefix,\n            )\n\n            # check whether loaded metadata file should be parsed as JSON\n            parse_json = (\n                not isinstance(schema_or_plugin, ValidationHandler)\n                or not schema_or_plugin._for_json\n            )\n            if parse_json:\n                # not a handler plugin for raw data -&gt; load as JSON\n                dat = curCtx.dirAdapter.decode_json(dat, metapath)\n                if dat is None:\n                    add_error(f\"File '{metapath}' could not be parsed\", key, metapath)\n                    continue\n\n            valErrs = validate_metadata(dat, schema_or_plugin)\n            if valErrs:\n                add_error(valErrs, key, metapath)\n\n        if curCtx.failed:\n            return False  # stop validation if primitive checks failed\n\n        # 3. check the complex constraints\n\n        # if-then-else\n        if rl.if_ is not None:\n            ifCtx = curCtx.descend(rl.if_, path, \"if\")\n            if self.validate_path(path, rl.if_, ifCtx):\n                if rl.then is not None:\n                    thenCtx = curCtx.descend(rl.then, path, \"then\")\n                    if not self.validate_path(path, rl.then, thenCtx):\n                        curCtx.failed = True\n                        # add_error(\"'if' rule satisfied, but 'then' rule violated\", \"then\")  # noqa: E501\n                        if rl.details:\n                            curCtx.add_errors(thenCtx.errors)\n            else:\n                if rl.else_ is not None:\n                    elseCtx = curCtx.descend(rl.else_, path, \"else\")\n                    if not self.validate_path(path, rl.else_, elseCtx):\n                        curCtx.failed = True\n                        # add_error(\"'if' rule violated and also 'else' rule violated\", \"else\")  # noqa: E501\n\n                        if rl.details:\n                            curCtx.add_errors(elseCtx.errors)\n\n        # logical operators\n        for op in (\"allOf\", \"anyOf\", \"oneOf\"):\n            val = rl.__dict__[op]\n            opCtx = curCtx.descend(rule, None, op)\n\n            num_rules = len(val)\n            if num_rules == 0:\n                continue  # empty list of rules -&gt; nothing to do\n\n            num_fails = 0\n            suberrs: List[DSValidationErrors] = []\n            for idx, r in enumerate(val):\n                subCtx = opCtx.descend(r, None, idx)\n                success = self.validate_path(path, r, subCtx)\n                if success and op == \"anyOf\":\n                    suberrs = []  # don't care about the errors on success\n                    break  # we have a satisfied rule -&gt; enough\n                elif not success:\n                    num_fails += 1\n                    if subCtx.errors:\n                        suberrs.append(subCtx.errors)\n\n            num_sat = num_rules - num_fails\n            err_msg = \"\"\n            if op == \"allOf\" and num_fails &gt; 0:\n                err_msg = \"All\"\n            elif op == \"oneOf\" and num_fails != num_rules - 1:\n                err_msg = \"Exactly 1\"\n            elif op == \"anyOf\" and num_fails == num_rules:\n                err_msg = \"At least 1\"\n            if err_msg:\n                err_msg += f\" of {num_rules} sub-rules must be satisfied \"\n                err_msg += f\"(satisfied: {num_sat})\"\n                add_error(err_msg, op, None)\n                if rl.details:\n                    curCtx.add_errors(*suberrs)\n\n        if rl.not_ is not None:\n            notCtx = curCtx.descend(rl.not_, path, \"not\")\n            if self.validate_path(path, rl.not_, notCtx):\n                add_error(\n                    \"Negated sub-rule satisfied, but should have failed\", \"not\", None\n                )\n\n        if curCtx.failed:\n            return False  # stop validation here if logical expressions failed\n\n        # 4. perform \"next\" rule, on possibly rewritten path\n        if rl.next is not None:\n            nextCtx = curCtx.descend(rl.next, nextPath, \"next\")\n            if not self.validate_path(nextPath, rl.next, nextCtx):\n                if rl.details:\n                    curCtx.add_errors(nextCtx.errors)\n                return False\n\n        # assert curCtx.failed == False\n        return True\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator.__init__","title":"__init__","text":"<pre><code>__init__(\n    schema: Union[bool, Rule, DSRule, str, Path],\n    meta_conv: Optional[MetaConvention] = None,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\",\n) -&gt; None\n</code></pre> <p>Construct validator instance from given schema or schema location.</p> <p>Accepts DSRule, raw bool or Rule, or a str/Path that is interpreted as location.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def __init__(\n    self,\n    schema: Union[bool, Rule, DSRule, str, Path],\n    meta_conv: Optional[MetaConvention] = None,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\",\n) -&gt; None:\n\"\"\"Construct validator instance from given schema or schema location.\n\n    Accepts DSRule, raw bool or Rule, or a str/Path that is interpreted as location.\n    \"\"\"\n    self.meta_conv = meta_conv or MetaConvention()\n    self.local_basedir = local_basedir\n    self.relative_prefix = relative_prefix\n\n    # if the passed relative prefix is a custom plugin, we cannot use this\n    # for $ref resolving, so we will ignore it in the Json/Yaml loader\n    is_plugin_prefix = relative_prefix.find(\"v#\") &lt; relative_prefix.find(\"://\") == 0\n\n    # take care of the passed schema based on its type\n    if isinstance(schema, bool) or isinstance(schema, Rule):\n        self.schema = DSRule(__root__=schema)\n    elif isinstance(schema, DSRule):\n        self.schema = schema\n    elif isinstance(schema, str) or isinstance(schema, Path):\n        uri = to_uri(str(schema), self.local_basedir, self.relative_prefix)\n        dat = load_json(\n            uri,\n            local_basedir=self.local_basedir,\n            relative_prefix=self.relative_prefix if not is_plugin_prefix else \"\",\n        )\n        # use deepcopy to get rid of jsonref (see jsonref issue #9)\n        # otherwise we will get problems with pydantic serialization later\n        self.schema = DSRule.parse_obj(copy.deepcopy(dat))\n    else:\n        raise ValueError(f\"Do not know how to process provided schema: {schema}\")\n\n    logger.debug(\n        \"Initialized dirschema validator\\n\"\n        f\"schema: {self.schema}\\n\"\n        f\"meta_conv: {self.meta_conv}\\n\"\n        f\"local_basedir: {self.local_basedir}\\n\"\n    )\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator.errors_to_json","title":"errors_to_json  <code>classmethod</code>","text":"<pre><code>errors_to_json(errs: DSValidationResult) -&gt; Dict[str, Any]\n</code></pre> <p>Convert the validation result to a JSON-compatible dict.</p> <p>Resulting structure is (file path -&gt; schema location -&gt; error message or dict).</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>@classmethod\ndef errors_to_json(cls, errs: DSValidationResult) -&gt; Dict[str, Any]:\n\"\"\"Convert the validation result to a JSON-compatible dict.\n\n    Resulting structure is (file path -&gt; schema location -&gt; error message or dict).\n    \"\"\"\n    return {\n        file_path: {\n            loc_to_jsonpointer(err_loc): json_dict(err_obj, exclude_defaults=True)\n            for err_loc, err_obj in file_errors.items()\n        }\n        for file_path, file_errors in errs.items()\n    }\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator.format_errors","title":"format_errors  <code>classmethod</code>","text":"<pre><code>format_errors(\n    errs: DSValidationResult,\n    stream: DSValidationResult = None,\n) -&gt; Optional[str]\n</code></pre> <p>Report errors as YAML output.</p> <p>If a stream is provided, prints it out. Otherwise, returns it as string.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>@classmethod\ndef format_errors(cls, errs: DSValidationResult, stream=None) -&gt; Optional[str]:\n\"\"\"Report errors as YAML output.\n\n    If a stream is provided, prints it out. Otherwise, returns it as string.\n    \"\"\"\n    of = stream or io.StringIO()\n    yaml.dump(cls.errors_to_json(errs), of)\n    if not stream:\n        return of.getvalue()\n    return None\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator.validate","title":"validate","text":"<pre><code>validate(\n    root_path: Union[Path, IDirectory],\n    **kwargs: Union[Path, IDirectory]\n) -&gt; DSValidationResult\n</code></pre> <p>Validate a directory, return all validation errors (unsatisfied rules).</p> <p>If <code>root_path</code> is an instance of <code>IDirectory</code>, it will be used directly.</p> <p>If <code>root_path</code> is a <code>Path</code>, this function will try to pick the correct interface for interpreting \"files\" and \"directories\", depending on whether the provided file is a directory or a supported kind of archive file with internal structure.</p> <p>Depending on the used metadata convention, the companion metadata files matching the convention will be filtered out from the set of validated paths.</p> <p>Returns     Error dict that is empty in case of success, or otherwise contains     for each path with validation errors another dict with the errors.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def validate(\n    self, root_path: Union[Path, IDirectory], **kwargs\n) -&gt; DSValidationResult:\n\"\"\"Validate a directory, return all validation errors (unsatisfied rules).\n\n    If `root_path` is an instance of `IDirectory`, it will be used directly.\n\n    If `root_path` is a `Path`, this function will try to pick the correct\n    interface for interpreting \"files\" and \"directories\", depending on\n    whether the provided file is a directory or a supported kind of archive\n    file with internal structure.\n\n    Depending on the used metadata convention, the companion metadata files\n    matching the convention will be filtered out from the set of validated\n    paths.\n\n    Returns\n        Error dict that is empty in case of success, or otherwise contains\n        for each path with validation errors another dict with the errors.\n    \"\"\"\n    logger.debug(f\"validate '{root_path}' ...\")\n    if isinstance(root_path, Path):\n        root_path = get_adapter_for(root_path)\n    paths = [p for p in root_path.get_paths() if not self.meta_conv.is_meta(p)]\n    errors: Dict[str, Any] = {}\n    # run validation for each filepath, collect errors separately\n    for p in paths:\n        ctx = DSEvalCtx.fresh(\n            self.schema,\n            dirAdapter=root_path,\n            metaConvention=self.meta_conv,\n            filePath=p,\n            **kwargs,\n        )\n        logger.debug(f\"validate_path '{p}' ...\")\n        success = self.validate_path(p, self.schema, ctx)\n        logger.debug(f\"validate_path '{p}' -&gt; {success}\")\n        if not success:\n            errors[p] = ctx.errors or {\n                (): DSValidationError(\n                    path=p, err=\"Validation failed (no error log available).\"\n                )\n            }\n    return errors\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.DSValidator.validate_path","title":"validate_path","text":"<pre><code>validate_path(\n    path: str, rule: DSRule, curCtx: DSEvalCtx\n) -&gt; bool\n</code></pre> <p>Apply rule to path of file/directory under given evaluation context.</p> <p>Will collect errors in the context object.</p> <p>Note that not all errors might be reported, as the sub-rules are evaluated in different stages and each stage aborts evaluation on failure (i.e. match/rewrite, primitive rules, complex logic rules, <code>next</code> sub-rule)</p> <p>Returns True iff validation of this rule was successful.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def validate_path(self, path: str, rule: DSRule, curCtx: DSEvalCtx) -&gt; bool:\n\"\"\"Apply rule to path of file/directory under given evaluation context.\n\n    Will collect errors in the context object.\n\n    Note that not all errors might be reported, as the sub-rules are\n    evaluated in different stages and each stage aborts evaluation on\n    failure (i.e. match/rewrite, primitive rules, complex logic rules,\n    `next` sub-rule)\n\n    Returns True iff validation of this rule was successful.\n    \"\"\"\n    logger.debug(f\"validate_path '{path}', at rule location: {curCtx.location}\")\n\n    # special case: trivial bool rule\n    if isinstance(rule.__root__, bool):\n        logger.debug(curCtx.location, \"trivial rule\")\n        if not rule.__root__:\n            curCtx.failed = True\n            curCtx.add_error(\"Reached unsatisfiable 'false' rule\")\n        return not curCtx.failed\n\n    rl = rule.__root__  # unpack rule\n    # assert isinstance(rl, Rule)\n\n    # 1. match / rewrite\n    # if rewrite is set, don't need to do separate match,just try rewriting\n    # match/rewrite does not produce an error on its own, but can fail\n    # because \"match failure\" is usually not \"validation failure\"\n    psl = PathSlice.into(path, curCtx.matchStart, curCtx.matchStop)\n    nextPath: str = path  # to be used for implication later on\n    if rl.match or rl.rewrite:\n        # important! using the match pattern from the context (could be inherited)\n        rewritten = psl.rewrite(curCtx.matchPat, rl.rewrite)\n        if rewritten is not None:\n            nextPath = rewritten.unslice()\n        else:  # failed match or rewrite\n            op = \"rewrite\" if rl.rewrite else \"match\"\n            pat = curCtx.matchPat or psl._def_pat\n            matchPat = f\"match '{pat.pattern}'\"\n            rwPat = f\" and rewrite to '{str(rl.rewrite)}'\" if rl.rewrite else \"\"\n\n            if rl.description:  # add custom error without expanding groups\n                curCtx.add_error(rl.description, op)\n            else:\n                curCtx.add_error(f\"Failed to {matchPat}{rwPat}\", op)\n            curCtx.failed = True\n            return False\n\n    # 2. proceed with the other primitive constraints\n\n    def add_error(*args):\n\"\"\"If desc is set, add desc error once and else add passed error.\"\"\"\n        if rl.description is None:\n            curCtx.add_error(*args)\n        elif rl.description != \"\" and not curCtx.failed:\n            # add error with expanded groups for better error messages\n            curCtx.add_error(psl.match(curCtx.matchPat).expand(rl.description))\n        curCtx.failed = True\n\n    # take care of type constraint\n    is_file = curCtx.dirAdapter.is_file(path)\n    is_dir = curCtx.dirAdapter.is_dir(path)\n    if rl.type is not None and not rl.type.is_satisfied(is_file, is_dir):\n        msg = f\"Entity does not have expected type: '{rl.type.value}'\"\n        if rl.type == TypeEnum.ANY:\n            msg = \"Entity must exist (type: true)\"\n        elif rl.type == TypeEnum.MISSING:\n            msg = \"Entity must not exist (type: false)\"\n        add_error(msg, \"type\", None)\n\n    # take care of metadata JSON Schema validation constraint\n    for key in (\"valid\", \"validMeta\"):\n        if rl.__dict__[key] is None:  # attribute not set\n            continue\n\n        if not is_file and not is_dir:\n            add_error(f\"Path '{path}' does not exist\", key, None)\n            continue\n\n        # use metadata convention for validMeta\n        metapath = path\n        if key == \"validMeta\":\n            metapath = curCtx.metaConvention.meta_for(path, is_dir=is_dir)\n\n        # load metadata file\n        dat = curCtx.dirAdapter.open_file(metapath)\n        if dat is None:\n            add_error(f\"File '{metapath}' could not be loaded\", key, metapath)\n            continue\n\n        # prepare correct validation method (JSON Schema or custom plugin)\n        schema_or_plugin = resolve_validator(\n            rl.__dict__[key],\n            local_basedir=self.local_basedir,\n            relative_prefix=self.relative_prefix,\n        )\n\n        # check whether loaded metadata file should be parsed as JSON\n        parse_json = (\n            not isinstance(schema_or_plugin, ValidationHandler)\n            or not schema_or_plugin._for_json\n        )\n        if parse_json:\n            # not a handler plugin for raw data -&gt; load as JSON\n            dat = curCtx.dirAdapter.decode_json(dat, metapath)\n            if dat is None:\n                add_error(f\"File '{metapath}' could not be parsed\", key, metapath)\n                continue\n\n        valErrs = validate_metadata(dat, schema_or_plugin)\n        if valErrs:\n            add_error(valErrs, key, metapath)\n\n    if curCtx.failed:\n        return False  # stop validation if primitive checks failed\n\n    # 3. check the complex constraints\n\n    # if-then-else\n    if rl.if_ is not None:\n        ifCtx = curCtx.descend(rl.if_, path, \"if\")\n        if self.validate_path(path, rl.if_, ifCtx):\n            if rl.then is not None:\n                thenCtx = curCtx.descend(rl.then, path, \"then\")\n                if not self.validate_path(path, rl.then, thenCtx):\n                    curCtx.failed = True\n                    # add_error(\"'if' rule satisfied, but 'then' rule violated\", \"then\")  # noqa: E501\n                    if rl.details:\n                        curCtx.add_errors(thenCtx.errors)\n        else:\n            if rl.else_ is not None:\n                elseCtx = curCtx.descend(rl.else_, path, \"else\")\n                if not self.validate_path(path, rl.else_, elseCtx):\n                    curCtx.failed = True\n                    # add_error(\"'if' rule violated and also 'else' rule violated\", \"else\")  # noqa: E501\n\n                    if rl.details:\n                        curCtx.add_errors(elseCtx.errors)\n\n    # logical operators\n    for op in (\"allOf\", \"anyOf\", \"oneOf\"):\n        val = rl.__dict__[op]\n        opCtx = curCtx.descend(rule, None, op)\n\n        num_rules = len(val)\n        if num_rules == 0:\n            continue  # empty list of rules -&gt; nothing to do\n\n        num_fails = 0\n        suberrs: List[DSValidationErrors] = []\n        for idx, r in enumerate(val):\n            subCtx = opCtx.descend(r, None, idx)\n            success = self.validate_path(path, r, subCtx)\n            if success and op == \"anyOf\":\n                suberrs = []  # don't care about the errors on success\n                break  # we have a satisfied rule -&gt; enough\n            elif not success:\n                num_fails += 1\n                if subCtx.errors:\n                    suberrs.append(subCtx.errors)\n\n        num_sat = num_rules - num_fails\n        err_msg = \"\"\n        if op == \"allOf\" and num_fails &gt; 0:\n            err_msg = \"All\"\n        elif op == \"oneOf\" and num_fails != num_rules - 1:\n            err_msg = \"Exactly 1\"\n        elif op == \"anyOf\" and num_fails == num_rules:\n            err_msg = \"At least 1\"\n        if err_msg:\n            err_msg += f\" of {num_rules} sub-rules must be satisfied \"\n            err_msg += f\"(satisfied: {num_sat})\"\n            add_error(err_msg, op, None)\n            if rl.details:\n                curCtx.add_errors(*suberrs)\n\n    if rl.not_ is not None:\n        notCtx = curCtx.descend(rl.not_, path, \"not\")\n        if self.validate_path(path, rl.not_, notCtx):\n            add_error(\n                \"Negated sub-rule satisfied, but should have failed\", \"not\", None\n            )\n\n    if curCtx.failed:\n        return False  # stop validation here if logical expressions failed\n\n    # 4. perform \"next\" rule, on possibly rewritten path\n    if rl.next is not None:\n        nextCtx = curCtx.descend(rl.next, nextPath, \"next\")\n        if not self.validate_path(nextPath, rl.next, nextCtx):\n            if rl.details:\n                curCtx.add_errors(nextCtx.errors)\n            return False\n\n    # assert curCtx.failed == False\n    return True\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.loc_to_jsonpointer","title":"loc_to_jsonpointer","text":"<pre><code>loc_to_jsonpointer(lst) -&gt; str\n</code></pre> <p>Convert a list of string keys and int indices to a JSON Pointer string.</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def loc_to_jsonpointer(lst) -&gt; str:\n\"\"\"Convert a list of string keys and int indices to a JSON Pointer string.\"\"\"\n    return \"/\" + \"/\".join(map(str, lst))\n</code></pre>"},{"location":"reference/dirschema/validate/#dirschema.validate.json_dict","title":"json_dict","text":"<pre><code>json_dict(model, **kwargs)\n</code></pre> <p>Given a Pydantic model, convert it to a raw JSON compatible dict.</p> <p>This uses a round-trip via JSON-serialization and deserialization to get rid of non-JSON entities (the <code>BaseModel.dict()</code> method yields possibly non-JSON dicts).</p> Source code in <code>src/dirschema/validate.py</code> <pre><code>def json_dict(model, **kwargs):\n\"\"\"Given a Pydantic model, convert it to a raw JSON compatible dict.\n\n    This uses a round-trip via JSON-serialization and deserialization to get rid\n    of non-JSON entities (the `BaseModel.dict()` method yields possibly non-JSON dicts).\n    \"\"\"\n    return json.loads(model.json(**kwargs))\n</code></pre>"},{"location":"reference/dirschema/json/","title":"json","text":"<p>Utilities for validating JSON.</p>"},{"location":"reference/dirschema/json/handler/","title":"handler","text":"<p>Interface for custom validation handlers.</p>"},{"location":"reference/dirschema/json/handler/#dirschema.json.handler.ValidationHandler","title":"ValidationHandler","text":"<p>             Bases: <code>ABC</code></p> <p>Interface for custom validators that can be registered via entrypoints.</p> <p>Only one of validate or validate_json may be implemented.</p> <p>These can be used instead of JSON Schemas inside a dirschema like this:</p> <p><code>validMeta: \"v#ENTRYPOINT://any args for validator, e.g. schema name\"</code></p> Source code in <code>src/dirschema/json/handler.py</code> <pre><code>class ValidationHandler(ABC):  # we don't use @abstractmethod on purpose # noqa: B024\n\"\"\"Interface for custom validators that can be registered via entrypoints.\n\n    Only one of validate or validate_json may be implemented.\n\n    These can be used instead of JSON Schemas inside a dirschema like this:\n\n    `validMeta: \"v#ENTRYPOINT://any args for validator, e.g. schema name\"`\n    \"\"\"\n\n    def __init__(self, args: str):\n\"\"\"Store passed arguments in instance.\"\"\"\n        self.args = args\n\n    @property\n    def _for_json(self) -&gt; bool:\n\"\"\"Return whether this handler is for JSON (i.e. overrides validate_json).\"\"\"\n        return type(self).validate_json != ValidationHandler.validate_json\n\n    def validate(self, data) -&gt; Dict[str, List[str]]:\n\"\"\"Run validation on passed metadata object.\"\"\"\n        if self._for_json:\n            return self.validate_json(data, self.args)\n        else:\n            return self.validate_raw(data, self.args)\n\n    # ----\n\n    @classmethod\n    def validate_raw(cls, data: IO[bytes], args: str) -&gt; Dict[str, List[str]]:\n\"\"\"Perform custom validation on passed raw binary stream.\n\n        This can be used to implement validators for files that are not JSON\n        or not parsable as JSON by the adapter used in combination with the handler.\n\n        Args:\n            data: Binary data stream\n            args: String following the entry-point prefix, i.e.\n                when used as `v#ENTRYPOINT://a` the `args` value will be \"a\".\n\n        Returns:\n            The output is a dict mapping from paths (JSON Pointers) inside the\n            object to respective collected error messages.\n\n            If there are no errors, an empty dict is returned.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def validate_json(cls, data: Any, args: str) -&gt; Dict[str, List[str]]:\n\"\"\"Perform custom validation on passed JSON dict.\n\n        Args:\n            data: Valid JSON dict loaded by a dirschema adapter.\n            args: String following the entry-point prefix, i.e.\n                when used as `v#ENTRYPOINT://a` the `args` value will be \"a\".\n\n        Returns:\n            The output is a dict mapping from paths (JSON Pointers) inside the\n            object to respective collected error messages.\n\n            If there are no errors, an empty dict is returned.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/dirschema/json/handler/#dirschema.json.handler.ValidationHandler.__init__","title":"__init__","text":"<pre><code>__init__(args: str)\n</code></pre> <p>Store passed arguments in instance.</p> Source code in <code>src/dirschema/json/handler.py</code> <pre><code>def __init__(self, args: str):\n\"\"\"Store passed arguments in instance.\"\"\"\n    self.args = args\n</code></pre>"},{"location":"reference/dirschema/json/handler/#dirschema.json.handler.ValidationHandler.validate","title":"validate","text":"<pre><code>validate(data) -&gt; Dict[str, List[str]]\n</code></pre> <p>Run validation on passed metadata object.</p> Source code in <code>src/dirschema/json/handler.py</code> <pre><code>def validate(self, data) -&gt; Dict[str, List[str]]:\n\"\"\"Run validation on passed metadata object.\"\"\"\n    if self._for_json:\n        return self.validate_json(data, self.args)\n    else:\n        return self.validate_raw(data, self.args)\n</code></pre>"},{"location":"reference/dirschema/json/handler/#dirschema.json.handler.ValidationHandler.validate_raw","title":"validate_raw  <code>classmethod</code>","text":"<pre><code>validate_raw(\n    data: IO[bytes], args: str\n) -&gt; Dict[str, List[str]]\n</code></pre> <p>Perform custom validation on passed raw binary stream.</p> <p>This can be used to implement validators for files that are not JSON or not parsable as JSON by the adapter used in combination with the handler.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>IO[bytes]</code> <p>Binary data stream</p> required <code>args</code> <code>str</code> <p>String following the entry-point prefix, i.e. when used as <code>v#ENTRYPOINT://a</code> the <code>args</code> value will be \"a\".</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>The output is a dict mapping from paths (JSON Pointers) inside the</p> <code>Dict[str, List[str]]</code> <p>object to respective collected error messages.</p> <code>Dict[str, List[str]]</code> <p>If there are no errors, an empty dict is returned.</p> Source code in <code>src/dirschema/json/handler.py</code> <pre><code>@classmethod\ndef validate_raw(cls, data: IO[bytes], args: str) -&gt; Dict[str, List[str]]:\n\"\"\"Perform custom validation on passed raw binary stream.\n\n    This can be used to implement validators for files that are not JSON\n    or not parsable as JSON by the adapter used in combination with the handler.\n\n    Args:\n        data: Binary data stream\n        args: String following the entry-point prefix, i.e.\n            when used as `v#ENTRYPOINT://a` the `args` value will be \"a\".\n\n    Returns:\n        The output is a dict mapping from paths (JSON Pointers) inside the\n        object to respective collected error messages.\n\n        If there are no errors, an empty dict is returned.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/dirschema/json/handler/#dirschema.json.handler.ValidationHandler.validate_json","title":"validate_json  <code>classmethod</code>","text":"<pre><code>validate_json(data: Any, args: str) -&gt; Dict[str, List[str]]\n</code></pre> <p>Perform custom validation on passed JSON dict.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Any</code> <p>Valid JSON dict loaded by a dirschema adapter.</p> required <code>args</code> <code>str</code> <p>String following the entry-point prefix, i.e. when used as <code>v#ENTRYPOINT://a</code> the <code>args</code> value will be \"a\".</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>The output is a dict mapping from paths (JSON Pointers) inside the</p> <code>Dict[str, List[str]]</code> <p>object to respective collected error messages.</p> <code>Dict[str, List[str]]</code> <p>If there are no errors, an empty dict is returned.</p> Source code in <code>src/dirschema/json/handler.py</code> <pre><code>@classmethod\ndef validate_json(cls, data: Any, args: str) -&gt; Dict[str, List[str]]:\n\"\"\"Perform custom validation on passed JSON dict.\n\n    Args:\n        data: Valid JSON dict loaded by a dirschema adapter.\n        args: String following the entry-point prefix, i.e.\n            when used as `v#ENTRYPOINT://a` the `args` value will be \"a\".\n\n    Returns:\n        The output is a dict mapping from paths (JSON Pointers) inside the\n        object to respective collected error messages.\n\n        If there are no errors, an empty dict is returned.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/dirschema/json/handler_pydantic/","title":"handler_pydantic","text":"<p>Minimal handler for using Pydantic models.</p>"},{"location":"reference/dirschema/json/handler_pydantic/#dirschema.json.handler_pydantic.PydanticHandler","title":"PydanticHandler","text":"<p>             Bases: <code>ValidationHandler</code></p> <p>Validation handler using <code>parse_obj</code> of pydantic models instead of JSON Schema.</p> <p>Can serve as a simple template for other handlers, or be subclassed to properly register your own models and used from your unique entry-point.</p> <p>In principle, you can also override/add to the <code>MODELS</code> of this class programmatically, but then you must accept the following disadvantages:</p> <ul> <li>your dirschema using this handler cannot be checked from the CLI</li> <li>your models are registered \"globally\", which might lead to collisions</li> </ul> Source code in <code>src/dirschema/json/handler_pydantic.py</code> <pre><code>class PydanticHandler(ValidationHandler):\n\"\"\"Validation handler using `parse_obj` of pydantic models instead of JSON Schema.\n\n    Can serve as a simple template for other handlers, or be subclassed\n    to properly register your own models and used from your unique entry-point.\n\n    In principle, you can also override/add to the `MODELS` of this class\n    programmatically, but then you must accept the following disadvantages:\n\n    * your dirschema using this handler cannot be checked from the CLI\n    * your models are registered \"globally\", which might lead to collisions\n    \"\"\"\n\n    MODELS: Dict[str, Type[BaseModel]] = {}\n\n    @classmethod\n    def validate_json(cls, metadata: Any, args: str) -&gt; Dict[str, List[str]]:\n\"\"\"See [dirschema.json.handler.ValidationHandler.validate_json][].\"\"\"\n        model: Optional[Type[BaseModel]] = cls.MODELS.get(args)\n        if model is None:\n            raise ValueError(f\"Unknown pydantic model: '{args}'\")\n        if not issubclass(model, BaseModel):\n            raise ValueError(f\"Invalid pydantic model: '{args}'\")\n        try:\n            model.parse_obj(metadata)\n        except ValidationError as e:\n            errs: Dict[str, List[str]] = {}\n            for verr in e.errors():\n                key = \"/\" + \"/\".join(map(str, verr[\"loc\"]))\n                if key not in errs:\n                    errs[key] = []\n                errs[key].append(verr[\"msg\"])\n            return errs\n        return {}\n</code></pre>"},{"location":"reference/dirschema/json/handler_pydantic/#dirschema.json.handler_pydantic.PydanticHandler.validate_json","title":"validate_json  <code>classmethod</code>","text":"<pre><code>validate_json(\n    metadata: Any, args: str\n) -&gt; Dict[str, List[str]]\n</code></pre> <p>See dirschema.json.handler.ValidationHandler.validate_json.</p> Source code in <code>src/dirschema/json/handler_pydantic.py</code> <pre><code>@classmethod\ndef validate_json(cls, metadata: Any, args: str) -&gt; Dict[str, List[str]]:\n\"\"\"See [dirschema.json.handler.ValidationHandler.validate_json][].\"\"\"\n    model: Optional[Type[BaseModel]] = cls.MODELS.get(args)\n    if model is None:\n        raise ValueError(f\"Unknown pydantic model: '{args}'\")\n    if not issubclass(model, BaseModel):\n        raise ValueError(f\"Invalid pydantic model: '{args}'\")\n    try:\n        model.parse_obj(metadata)\n    except ValidationError as e:\n        errs: Dict[str, List[str]] = {}\n        for verr in e.errors():\n            key = \"/\" + \"/\".join(map(str, verr[\"loc\"]))\n            if key not in errs:\n                errs[key] = []\n            errs[key].append(verr[\"msg\"])\n        return errs\n    return {}\n</code></pre>"},{"location":"reference/dirschema/json/handlers/","title":"handlers","text":"<p>Loading of the validation handlers found in the current environment.</p>"},{"location":"reference/dirschema/json/handlers/#dirschema.json.handlers.loaded_handlers","title":"loaded_handlers  <code>module-attribute</code>","text":"<pre><code>loaded_handlers = {\n    ep.name: ep.load()\n    for ep in entrypoints.get_group_all(\n        group=\"dirschema_validator\"\n    )\n}\n</code></pre> <p>Dict mapping from registered ValidationHandlers to the corresponding classes.</p>"},{"location":"reference/dirschema/json/parse/","title":"parse","text":"<p>Helper functions to allow using JSON and YAML interchangably + take care of $refs.</p>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.ExtJsonLoader","title":"ExtJsonLoader","text":"<p>             Bases: <code>JsonLoader</code></p> <p>Extends JsonLoader with capabilities.</p> <p>Adds support for:</p> <ul> <li>loading YAML</li> <li>resolving relative paths</li> </ul> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>class ExtJsonLoader(JsonLoader):\n\"\"\"Extends JsonLoader with capabilities.\n\n    Adds support for:\n\n    * loading YAML\n    * resolving relative paths\n    \"\"\"\n\n    def __init__(\n        self, *, local_basedir: Optional[Path] = None, relative_prefix: str = \"\"\n    ):\n\"\"\"Initialize loader with URI resolution arguments.\"\"\"\n        super().__init__()\n        self.local_basedir = local_basedir\n        self.rel_prefix = relative_prefix\n\n    def __call__(self, uri: str, **kwargs):\n\"\"\"Try loading passed uri as YAML if loading as JSON fails.\"\"\"\n        uri = to_uri(uri, self.local_basedir, self.rel_prefix)  # normalize path/uri\n        try:\n            return super().__call__(uri, **kwargs)\n        except json.JSONDecodeError:\n            strval = urlopen(uri).read().decode(\"utf-8\")  # noqa: S310\n            res = yaml.load(io.StringIO(strval, **kwargs))\n            if self.cache_results:\n                self.store[uri] = res\n            return res\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.ExtJsonLoader.__init__","title":"__init__","text":"<pre><code>__init__(\n    *,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\"\n)\n</code></pre> <p>Initialize loader with URI resolution arguments.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def __init__(\n    self, *, local_basedir: Optional[Path] = None, relative_prefix: str = \"\"\n):\n\"\"\"Initialize loader with URI resolution arguments.\"\"\"\n    super().__init__()\n    self.local_basedir = local_basedir\n    self.rel_prefix = relative_prefix\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.ExtJsonLoader.__call__","title":"__call__","text":"<pre><code>__call__(uri: str, **kwargs: str)\n</code></pre> <p>Try loading passed uri as YAML if loading as JSON fails.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def __call__(self, uri: str, **kwargs):\n\"\"\"Try loading passed uri as YAML if loading as JSON fails.\"\"\"\n    uri = to_uri(uri, self.local_basedir, self.rel_prefix)  # normalize path/uri\n    try:\n        return super().__call__(uri, **kwargs)\n    except json.JSONDecodeError:\n        strval = urlopen(uri).read().decode(\"utf-8\")  # noqa: S310\n        res = yaml.load(io.StringIO(strval, **kwargs))\n        if self.cache_results:\n            self.store[uri] = res\n        return res\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.to_uri","title":"to_uri","text":"<pre><code>to_uri(\n    path: str,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\",\n) -&gt; str\n</code></pre> <p>Given a path or URI, normalize it to an absolute path.</p> <p>If the path is relative and without protocol, it is prefixed with <code>relative_prefix</code> before attempting to resolve it (by default equal to prepending <code>cwd://</code>)</p> <p>If path is already http(s):// or file://... path, do nothing to it. If the path is absolute (starts with a slash), just prepend file:// If the path is cwd://, resolve based on CWD (even if starting with a slash) If the path is local://, resolve based on <code>local_basedir</code> (if missing, CWD is used)</p> <p>Result is either http(s):// or a file:// path that can be read with urlopen.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def to_uri(\n    path: str, local_basedir: Optional[Path] = None, relative_prefix: str = \"\"\n) -&gt; str:\n\"\"\"Given a path or URI, normalize it to an absolute path.\n\n    If the path is relative and without protocol, it is prefixed with `relative_prefix`\n    before attempting to resolve it (by default equal to prepending `cwd://`)\n\n    If path is already http(s):// or file://... path, do nothing to it.\n    If the path is absolute (starts with a slash), just prepend file://\n    If the path is cwd://, resolve based on CWD (even if starting with a slash)\n    If the path is local://, resolve based on `local_basedir` (if missing, CWD is used)\n\n    Result is either http(s):// or a file:// path that can be read with urlopen.\n    \"\"\"\n    local_basedir = local_basedir or Path(\"\")\n    if str(path)[0] != \"/\" and str(path).find(\"://\") &lt; 0:\n        path = relative_prefix + path\n\n    prot, rest = \"\", \"\"\n    prs = str(path).split(\"://\")\n    if len(prs) == 1:\n        rest = prs[0]\n    else:\n        prot, rest = prs\n\n    if prot.startswith((\"http\", \"file\")):\n        return path  # nothing to do\n    elif prot == \"local\":\n        # relative, but not to CWD, but a custom path\n        rest = str((local_basedir / rest.lstrip(\"/\")).absolute())\n    elif prot == \"cwd\":\n        # like normal resolution of relative,\n        # but absolute paths are still interpreted relative,\n        # so cwd:// and cwd:/// are lead to the same results\n        rest = str((Path(rest.lstrip(\"/\"))).absolute())\n    elif prot == \"\":\n        # relative paths are made absolute\n        if not Path(rest).is_absolute():\n            rest = str((Path(rest)).absolute())\n    else:\n        raise ValueError(f\"Unknown protocol: {prot}\")\n\n    return f\"file://{rest}\"\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.loads_json_or_yaml","title":"loads_json_or_yaml","text":"<pre><code>loads_json_or_yaml(dat: str)\n</code></pre> <p>Parse a JSON or YAML object from a string.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def loads_json_or_yaml(dat: str):\n\"\"\"Parse a JSON or YAML object from a string.\"\"\"\n    try:\n        return json.loads(dat)\n    except json.JSONDecodeError:\n        return yaml.load(io.StringIO(dat))\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.init_loader","title":"init_loader","text":"<pre><code>init_loader(kwargs)\n</code></pre> <p>Initialize JSON/YAML loader from passed kwargs dict, removing its arguments.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def init_loader(kwargs):\n\"\"\"Initialize JSON/YAML loader from passed kwargs dict, removing its arguments.\"\"\"\n    return ExtJsonLoader(\n        local_basedir=kwargs.pop(\"local_basedir\", None),\n        relative_prefix=kwargs.pop(\"relative_prefix\", \"\"),\n    )\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.loads_json","title":"loads_json","text":"<pre><code>loads_json(dat: str, **kwargs: str) -&gt; Dict[str, Any]\n</code></pre> <p>Load YAML/JSON from a string, resolving all refs, both local and remote.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def loads_json(dat: str, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"Load YAML/JSON from a string, resolving all refs, both local and remote.\"\"\"\n    ldr = init_loader(kwargs)\n    return JsonRef.replace_refs(loads_json_or_yaml(dat), loader=ldr, **kwargs)\n</code></pre>"},{"location":"reference/dirschema/json/parse/#dirschema.json.parse.load_json","title":"load_json","text":"<pre><code>load_json(uri: str, **kwargs: str) -&gt; Dict[str, Any]\n</code></pre> <p>Load YAML/JSON from file/network + resolve all refs, both local and remote.</p> Source code in <code>src/dirschema/json/parse.py</code> <pre><code>def load_json(uri: str, **kwargs) -&gt; Dict[str, Any]:\n\"\"\"Load YAML/JSON from file/network + resolve all refs, both local and remote.\"\"\"\n    ldr = init_loader(kwargs)\n    return JsonRef.replace_refs(ldr(str(uri)), loader=ldr, **kwargs)\n</code></pre>"},{"location":"reference/dirschema/json/validate/","title":"validate","text":"<p>Helper functions to perform validation of JSON-compatible metadata files.</p>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.JSONValidationErrors","title":"JSONValidationErrors  <code>module-attribute</code>","text":"<pre><code>JSONValidationErrors = Dict[str, List[str]]\n</code></pre> <p>JSON validation errors mapping from JSON Pointers to of error message lists.</p>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.plugin_from_uri","title":"plugin_from_uri","text":"<pre><code>plugin_from_uri(custom_uri: str) -&gt; ValidationHandler\n</code></pre> <p>Parse a validation plugin pseudo-URI, return the plugin class and args string.</p> Source code in <code>src/dirschema/json/validate.py</code> <pre><code>def plugin_from_uri(custom_uri: str) -&gt; ValidationHandler:\n\"\"\"Parse a validation plugin pseudo-URI, return the plugin class and args string.\"\"\"\n    try:\n        if not custom_uri.startswith(\"v#\"):\n            raise ValueError\n        ep, args = custom_uri[2:].split(\"://\")\n        if ep == \"\":\n            raise ValueError\n    except ValueError:\n        msg = f\"Invalid custom validator plugin pseudo-URI: '{custom_uri}'\"\n        raise ValueError(msg) from None\n\n    try:\n        h: Type[ValidationHandler] = loaded_handlers[ep]\n        return h(args)\n    except KeyError:\n        raise ValueError(f\"Validator entry-point not found: '{ep}'\") from None\n</code></pre>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.validate_custom","title":"validate_custom","text":"<pre><code>validate_custom(\n    dat, plugin_str: str\n) -&gt; JSONValidationErrors\n</code></pre> <p>Perform validation based on a validation handler string.</p> Source code in <code>src/dirschema/json/validate.py</code> <pre><code>def validate_custom(dat, plugin_str: str) -&gt; JSONValidationErrors:\n\"\"\"Perform validation based on a validation handler string.\"\"\"\n    h = plugin_from_uri(plugin_str)\n    if h._for_json:\n        return h.validate_json(dat, h.args)\n    else:\n        return h.validate_raw(dat, h.args)\n</code></pre>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.validate_jsonschema","title":"validate_jsonschema","text":"<pre><code>validate_jsonschema(\n    dat, schema: Union[bool, Dict]\n) -&gt; JSONValidationErrors\n</code></pre> <p>Perform validation of a dict based on a JSON Schema.</p> Source code in <code>src/dirschema/json/validate.py</code> <pre><code>def validate_jsonschema(dat, schema: Union[bool, Dict]) -&gt; JSONValidationErrors:\n\"\"\"Perform validation of a dict based on a JSON Schema.\"\"\"\n    v = Draft202012Validator(schema=schema)  # type: ignore\n    errs: Dict[str, List[str]] = {}\n    for verr in sorted(v.iter_errors(dat), key=lambda e: e.path):  # type: ignore\n        key = \"/\" + \"/\".join(map(str, verr.path))  # JSON Pointer into document\n        if key not in errs:\n            errs[key] = []\n        errs[key].append(verr.message)\n    return errs\n</code></pre>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.resolve_validator","title":"resolve_validator","text":"<pre><code>resolve_validator(\n    schema_or_ref: Union[bool, str, Dict],\n    *,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\"\n) -&gt; Union[bool, Dict, ValidationHandler]\n</code></pre> <p>Resolve passed object into a schema or validator.</p> <p>If passed object is already a schema, will return it. If passed object is a string, will load the referenced schema or instantiate the custom validator (a string starting with <code>v#</code>).</p> Source code in <code>src/dirschema/json/validate.py</code> <pre><code>def resolve_validator(\n    schema_or_ref: Union[bool, str, Dict],\n    *,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\",\n) -&gt; Union[bool, Dict, ValidationHandler]:\n\"\"\"Resolve passed object into a schema or validator.\n\n    If passed object is already a schema, will return it.\n    If passed object is a string, will load the referenced schema\n    or instantiate the custom validator (a string starting with `v#`).\n    \"\"\"\n    if isinstance(schema_or_ref, bool) or isinstance(schema_or_ref, dict):\n        # embedded schema\n        return schema_or_ref\n\n    if not schema_or_ref.startswith(\"v#\"):\n        # load schema from URI\n        uri = to_uri(schema_or_ref, local_basedir, relative_prefix)\n        return load_json(uri, local_basedir=local_basedir)\n\n    # custom validation, not json schema\n    return plugin_from_uri(schema_or_ref)\n</code></pre>"},{"location":"reference/dirschema/json/validate/#dirschema.json.validate.validate_metadata","title":"validate_metadata","text":"<pre><code>validate_metadata(\n    dat,\n    schema: Union[bool, str, Dict, ValidationHandler],\n    *,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\"\n) -&gt; JSONValidationErrors\n</code></pre> <p>Validate object (dict or byte stream) using JSON Schema or custom validator.</p> <p>The validator must be either a JSON Schema dict, or a string pointing to a JSON Schema, or a custom validator handler string.</p> <p>Returns a dict mapping from JSON Pointers to a list of errors in that location. If the dict is empty, no validation errors were detected.</p> Source code in <code>src/dirschema/json/validate.py</code> <pre><code>def validate_metadata(\n    dat,\n    schema: Union[bool, str, Dict, ValidationHandler],\n    *,\n    local_basedir: Optional[Path] = None,\n    relative_prefix: str = \"\",\n) -&gt; JSONValidationErrors:\n\"\"\"Validate object (dict or byte stream) using JSON Schema or custom validator.\n\n    The validator must be either a JSON Schema dict, or a string\n    pointing to a JSON Schema, or a custom validator handler string.\n\n    Returns a dict mapping from JSON Pointers to a list of errors in that location.\n    If the dict is empty, no validation errors were detected.\n    \"\"\"\n    if isinstance(schema, str):\n        val = resolve_validator(\n            schema, local_basedir=local_basedir, relative_prefix=relative_prefix\n        )\n    else:\n        val = schema\n\n    if isinstance(val, ValidationHandler):\n        return val.validate(dat)\n    else:\n        return validate_jsonschema(dat, val)\n</code></pre>"}]}